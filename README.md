# Flink

â€‹	Apache Flink is a *framework*(æ¡†æ¶) and *distributed*(åˆ†å¸ƒå¼) processing engine for *stateful*(çŠ¶æ€) computations over *unbounded*(æ— ç•Œ) and *bounded data streams* [æ•°æ®æµä¸Šçš„æœ‰çŠ¶æ€è®¡ç®—]

## Flinkç®€ä»‹

#### åº”ç”¨åœºæ™¯

* ç”µå•†å’Œå¸‚åœºè¥é”€ï¼šå®æ—¶æŠ¥è¡¨ã€å¹¿å‘ŠæŠ•æ”¾ã€å®æ—¶æ¨è
* ç‰©è”ç½‘ï¼šå®æ—¶æ•°æ®é‡‡é›†ã€å®æ—¶æŠ¥è­¦
* ç‰©æµé…é€åŠæœåŠ¡ï¼šè®¢å•çŠ¶æ€è¿½è¸ªåŠæœåŠ¡
* é“¶è¡Œå’Œé‡‘èä¸šï¼šå®æ—¶ç»“ç®—ã€é£é™©æ£€æµ‹

#### ä¼˜åŠ¿

* æµæ‰¹ä¸€ä½“
* ä½å»¶è¿Ÿã€é«˜åå
* ç»“æœå‡†ç¡®æ€§å’Œè‰¯å¥½å®¹é”™æ€§

#### Flinkåˆ†å±‚çš„API

1. SQL [æœ€é«˜å±‚è¯­è¨€]
2. Table API [å£°æ˜å¼é¢†åŸŸä¸“ç”¨è¯­è¨€] (ç±»ä¼¼DSL)
3. `DataStream` / DataSet(å¼ƒç”¨è¾¹ç¼˜) API [æ ¸å¿ƒAPI]
4. æœ‰çŠ¶æ€æµå¤„ç† [åº•å±‚API] `ProcessFunction`

#### Sparkå’ŒFlink

| å¯¹æ¯”     | Spark               | Flink                            |
| -------- | ------------------- | -------------------------------- |
| æ¶æ„è®¾è®¡ | Lambda              | Kappa                            |
| åº•å±‚è®¾è®¡ | åŸºäºæ‰¹å¤„ç†è®¾è®¡      | åŸºäºæµå¤„ç†è®¾è®¡                   |
| æ•°æ®æ¨¡å‹ | RDD - å°æ•°æ®é‡é›†åˆ  | æ•°æ®æµ(DataStream)ã€äº‹ä»¶åºåˆ—     |
| è¿è¡Œæ¶æ„ | æ‰¹è®¡ç®—ï¼Œä»¥Stageåˆ’åˆ† | æ•°æ®ç›´æ¥å‘é€åˆ°ç›¸åº”çš„ç‚¹ï¼Œæ— éœ€ç­‰å¾… |

## Flinkå…¥é—¨æ¡ˆä¾‹

### Batch

```java
// 1.è·å–ç¯å¢ƒ
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

// 2.è·å–æ•°æ®æºã€è¯»å–æ•°æ®
DataSource<String> lineSource = env.readTextFile("input/words.txt");

// 3.é€šè¿‡lambdaè¡¨è¾¾å¼ï¼Œå°†lineSourceè½¬æ¢æˆTuple2æ ¼å¼ç„¶åæ”¶é›†èµ·æ¥ï¼Œæ‰å¹³æ˜ å°„å‡ºæ¥
FlatMapOperator<String, Tuple2<String, Long>> tuple2Return = lineSource.flatMap(
    (String line, Collector<Tuple2<String, Long>> out) -> {
        String[] str = line.split(" ");
        for (String s : str) {
            out.collect(Tuple2.of(s, 1L));
        }
    }
).returns(Types.TUPLE(Types.STRING, Types.LONG));

// 4.é€šè¿‡Tupleä¸­çš„ `0å·å…ƒç´ (key)` å¯¹ `1å·å…ƒç´ (value)` è¿›è¡ŒSumèšåˆç»Ÿè®¡
AggregateOperator<Tuple2<String, Long>> sum = tuple2Return.groupBy(0).sum(1);

// 5.æ‰“å°å‡ºç»“æœ
sum.print();
```

### Stream-Bounded

```java
// 1.è·å–ç¯å¢ƒ ã€readFileã€‘
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// 2.è·å–æ•°æ®æºã€è¯»å–æ•°æ®
DataStreamSource<String> lineSource = env.readTextFile("input/");

// 3.é€šè¿‡lambdaè¡¨è¾¾å¼ï¼Œå°†lineSourceè½¬æ¢æˆTuple2æ ¼å¼ç„¶åæ”¶é›†èµ·æ¥ï¼Œæ‰å¹³æ˜ å°„å‡ºæ¥
SingleOutputStreamOperator<Tuple2<String, Long>> tuple2Return = lineSource.flatMap(
    (String line, Collector<Tuple2<String, Long>> out) -> {
        String[] str = line.split(" ");
        for (String s : str) {
            out.collect(Tuple2.of(s, 1L));
        }
    }
).returns(Types.TUPLE(Types.STRING, Types.LONG));

// 4.é€šè¿‡Tupleä¸­çš„ `0å·å…ƒç´ (key)` å¯¹ `1å·å…ƒç´ (value)` è¿›è¡ŒSumèšåˆç»Ÿè®¡
SingleOutputStreamOperator<Tuple2<String, Long>> sum = tuple2Return.keyBy(data -> data.f0).sum(1);
// 5.æ‰“å°å‡ºç»“æœ
sum.print();

env.execute();
```

### Stream-unBounded

```java
// 1.è·å–ç¯å¢ƒ ã€socketã€‘
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// 2.è·å–æ•°æ®æºã€è¯»å–æ•°æ®
DataStreamSource<String> lineSource = env.socketTextStream("192.168.32.151", 11778);

// 3.é€šè¿‡lambdaè¡¨è¾¾å¼ï¼Œå°†lineSourceè½¬æ¢æˆTuple2æ ¼å¼ç„¶åæ”¶é›†èµ·æ¥ï¼Œæ‰å¹³æ˜ å°„å‡ºæ¥
SingleOutputStreamOperator<Tuple2<String, Long>> tuple2Return = lineSource.flatMap(
    (String line, Collector<Tuple2<String, Long>> out) -> {
        String[] str = line.split(" ");
        for (String s : str) {
            out.collect(Tuple2.of(s, 1L));
        }
    }
).returns(Types.TUPLE(Types.STRING, Types.LONG));

// 4.é€šè¿‡Tupleä¸­çš„ `0å·å…ƒç´ (key)` å¯¹ `1å·å…ƒç´ (value)` è¿›è¡ŒSumèšåˆç»Ÿè®¡
SingleOutputStreamOperator<Tuple2<String, Long>> sum = tuple2Return.keyBy(data -> data.f0).sum(1);
// 5.æ‰“å°å‡ºç»“æœ
sum.print();

env.execute();
```

## Flinkç¯å¢ƒæ­å»º

### ä¸Šä¼ èµ„æºï¼Œå®‰è£…java11ç¯å¢ƒ

> ä¸Šä¼  `flink-1.15.3-bin-scala_2.12.tgz` + `jdk-11.0.15.1_linux-x64_bin.tar.gz` åˆ° `/export/server` ç›®å½•ä¸‹
>
> å®‰è£… `Java11` å¹¶è§£å‹ `Flink1.15.3` 
>
> æ–°å»º flink ç”¨æˆ·ï¼Œä¿®æ”¹ flink ç”¨æˆ·çš„ `JAVA_HOME` ä¸ºJava11

### Standalone

ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­

```yaml
# ä¿®æ”¹jobmanagerçš„ä¸»èŠ‚ç‚¹åœ¨node1ä¸Š
jobmanager.rpc.address: node1
# ç»‘å®šæ‰€æœ‰ä¸»æœºéƒ½å¯ä»¥è®¿é—®æœ¬æœº
jobmanager.bind-host: 0.0.0.0

# é»˜è®¤webuiç»‘å®šåœ¨node1:8081ä¸Š ã€å¯æ³¨é‡Šã€‘
rest.port: 8081
rest.bind-address: node1
```

```shell
# å¯åŠ¨é›†ç¾¤
$FLINK_HOME/bin/start-cluster.sh

# åœæ­¢é›†ç¾¤
$FLINK_HOME/bin/stop-cluster.sh
```

### éƒ¨ç½²æ¨¡å¼

#### Session

ä¼šè¯æ¨¡å¼ï¼Œç‰¹ç‚¹æ˜¯èµ„æºå›ºå®šï¼Œä¸éœ€è¦é‡å¤å¯åŠ¨é›†ç¾¤å’Œèµ„æºï¼Œé€‚ç”¨äº`ä»»åŠ¡æ‰§è¡Œæ—¶é—´çŸ­ï¼Œä½œä¸šæ•°é‡å¤š`çš„åœºæ™¯ [standalone]

#### Per-Job

å•ä»»åŠ¡æ¨¡å¼ï¼Œç‰¹ç‚¹æ˜¯æ¯`ä¸€ä¸ªé›†ç¾¤åªé’ˆå¯¹ä¸€ä¸ªä»»åŠ¡`è¿›è¡Œè¿è¡Œï¼Œé€‚ç”¨äºä»»åŠ¡æ‰§è¡Œæ—¶é—´é•¿ï¼Œä½œä¸šæ•°é‡å°‘çš„åœºæ™¯ [yarnã€k8s]

#### Application

åº”ç”¨æ¨¡å¼ï¼Œç‰¹ç‚¹æ˜¯æ¯`ä¸€ä¸ªé›†ç¾¤åªé’ˆå¯¹ä¸€ä¸ªåº”ç”¨ç¨‹åº`è¿›è¡Œè¿è¡Œï¼Œé€‚ç”¨äºä»»åŠ¡æ‰§è¡Œæ—¶é—´é•¿ï¼Œä½œä¸šæ•°é‡å°‘çš„åœºæ™¯ [yarnã€k8s]

> other: `standaloneåœ¨applicationéƒ¨ç½²ä¸­`ï¼Œå¯ä»¥ä½¿ç”¨åŒæ ·åœ¨binç›®å½•ä¸‹çš„standalone-job.shæ¥åˆ›å»ºä¸€ä¸ªJobManager
>
> 1. è¿›å…¥åˆ°Flinkçš„å®‰è£…è·¯å¾„ä¸‹ï¼Œ`å°†åº”ç”¨ç¨‹åºçš„jaråŒ…æ”¾åˆ°lib/ç›®å½•ä¸‹`
>
> 	```shell
> 	cp ./FlinkTutorial-1.0-SNAPSHOT.jar lib/
> 	```
>
> 2. å¯åŠ¨JobManager
>
> 	```shell
> 	./bin/standalone-job.sh start --job-classname jarPath
> 	# ç›´æ¥æŒ‡å®šä½œä¸šå…¥å£ï¼Œè„šæœ¬ä¼šç›´æ¥æ‰«ælibç›®å½•ï¼Œæ‰€æœ‰çš„jaråŒ…
> 	```
>
> 3. å¯åŠ¨TaskManager
>
> 	```shell
> 	./bin/taskmanager.sh start
> 	```
>
> 4. åœæ­¢é›†ç¾¤
>
> 	```shell
> 	./bin/standalone-job.sh stop
> 	./bin/taskmanager.sh stop
> 	```

### Yarn

#### Session mode

åœ¨Yarné›†ç¾¤ä¸­å¼€å¯ä¸€ä¸ªé›†ç¾¤ï¼Œå¼€å¯Yarnä¼šè¯

```shell
bin/yarn-session.sh -nm appName
```

| å‚æ•°                     | å¤‡æ³¨                        |
| ------------------------ | --------------------------- |
| -d                       | åˆ†ç¦»æ¨¡å¼ï¼Œåå°è¿è¡Œ          |
| -jm(--jobManagerMemory)  | é…ç½®JobManagerå†…å­˜ï¼Œé»˜è®¤MB  |
| -nm(--name)              | é…ç½®YARN UIç•Œé¢çš„ä»»åŠ¡å     |
| -qu(--queue)             | æŒ‡å®šYARN é˜Ÿåˆ—å             |
| -tm(--taskManagerMemory) | é…ç½®TaskManagerå†…å­˜ï¼Œé»˜è®¤MB |

#### Per-Job mode

åœ¨Yarné›†ç¾¤ä¸­å¼€å¯ä¸€ä¸ªé›†ç¾¤ï¼Œç”¨äºæäº¤ä¸€ä¸ªå•ç‹¬çš„ä½œä¸šï¼Œå¯åŠ¨Flinké›†ç¾¤

```shell
# ç°åœ¨çš„ç‰ˆæœ¬
bin/flink run -d -t yarn-per-job -c top.taurushu.wc.WordCount FlinkWordCount.jar

# ä»¥å‰çš„ç‰ˆæœ¬ 
bin/flink run -m yarn-cluster -c top.taurushu.wc.WordCount FlinkWordCount.jar
```

| å‚æ•°                   | å¤‡æ³¨                                 |
| ---------------------- | ------------------------------------ |
| -d                     | åˆ†ç¦»æ¨¡å¼ï¼Œåå°è¿è¡Œ                   |
| -t                     | è¿è¡Œæ¨¡å¼ "yarn-per-job" (deprecated) |
| -c,--class <classname> | é…ç½®å¯åŠ¨ç±»å                         |

#### Application mode

åœ¨Yarné›†ç¾¤ä¸­å¼€å¯ä¸€ä¸ªé›†ç¾¤ï¼Œç”¨äºæäº¤ä¸€ä¸ªåº”ç”¨ç¨‹åºï¼Œå¯åŠ¨Flinké›†ç¾¤

```shell
# å¼€å¯é›†ç¾¤ï¼Œæäº¤ä½œä¸š
bin/flink run-application -t yarn-application -c top.taurushu.wc.WordCount FlinkWordCount.jar
# æŸ¥çœ‹é›†ç¾¤
bin/flink list -t yarn-application -Dyarn.application.id=application_xxxxxx_yy
# å–æ¶ˆä½œä¸š
bin/flink canel -t yarn-application -Dyarn.application.id=application_xxxxxx_yy <jobId>
```

æŠ¥é”™é—®é¢˜ï¼š

![image-20230402233336305](png/iak8pp.png)

> Exception in thread "Thread-5" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
> 
> è§£å†³æ–¹æ¡ˆï¼šåœ¨conf/flink-config.propertiesä¸­é…ç½®classloader.check-leaked-classloader: false

## ç³»ç»Ÿæ¶æ„

### Flinkè¿è¡Œæ—¶æ¶æ„

![jobæäº¤æŠ½è±¡æµç¨‹](png/tzicng-1.png)

### ä½œä¸šæäº¤æµç¨‹

#### jobæäº¤æŠ½è±¡æµç¨‹

![jobæäº¤æŠ½è±¡æµç¨‹](png/tuqhe3-1.png)

#### Standaloneæäº¤æµç¨‹

<img src="png/tzp5qu-1.png" alt="Standaloneæäº¤æµç¨‹" style="zoom: 33%;" />

#### Yarn-sessionæäº¤æµç¨‹

<img src="png/txns4p-1.png" alt="Yarn-sessionæäº¤æµç¨‹" style="zoom: 50%;" />

#### Yarn-Per-Jobæäº¤æµç¨‹

<img src="png/txxfe0-1.png" alt="Yarn-Per-Jobæäº¤æµç¨‹" style="zoom:50%;" />

### ç¨‹åºå’Œæ•°æ®æµ(DataFlow)

ä¸€ä¸ªFlinkç¨‹åºå¯ä»¥çœ‹ä½œä¸‰éƒ¨åˆ†ï¼šsourceã€transformationã€sink

Flinkç¨‹åºåœ¨è¿è¡Œæ—¶ï¼Œä¼šè¢«è½¬æ¢ä¸ºé€»è¾‘æ•°æ®æµ(DataFlows)åŒ…å«äº†ä¸‰éƒ¨åˆ†ï¼Œä»¥ä¸€ä¸ªæˆ–å¤šä¸ªsourceå¼€å§‹ï¼Œä»¥ä¸€ä¸ªæˆ–å¤šä¸ªsinkç»“æŸã€‚

### å¹¶è¡Œåº¦(Parallelism)

å¹¶è¡Œåº¦åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š`æ•°æ®å¹¶è¡Œåº¦`å’Œ`ä»»åŠ¡å¹¶è¡Œåº¦`

è®¾ç½®å¹¶è¡Œåº¦çš„æ–¹å¼:

```java
// 1. flink-config.properties
// parallelism.default: 1

// 2. å‘½ä»¤è¡Œå‚æ•°è®¾ç½®
// Parallelism=2

// 3. ä»£ç ç¯å¢ƒè®¾ç½®
env.setParallelism(2)
    
// 4. ä»£ç è¿è¡Œæ—¶æ·»åŠ ç®—å­å¹¶è¡Œåº¦ï¼Œä¼˜å…ˆçº§æœ€é«˜
Operator().setParallelism(2)
```

#### æ•°æ®ä¼ è¾“å½¢å¼

1. ä¸€ä¸ªç¨‹åºä¸­ï¼Œä¼šåŒ…å«å¤šä¸ªç®—å­ï¼Œä¸åŒçš„ç®—å­ä¼šå­˜åœ¨ä¸åŒçš„å¹¶è¡Œåº¦
2. ç®—å­é—´ï¼Œæ•°æ®å¯ä»¥æ˜¯one-to-one(forwarding)ï¼Œä¹Ÿå¯ä»¥æ˜¯redistributingï¼Œå–å†³äºç®—å­ç§ç±»
	* one-to-one(forwarding)ï¼šæ„å‘³ç€ï¼Œä¸¤ä¸ªç®—å­ä¹‹é—´ç»´æŠ¤çš„å…ƒç´ ä¸ªæ•°ã€é¡ºåºéƒ½ç›¸åŒï¼Œæ˜¯ä¸€å¯¹ä¸€å…³ç³»
	* redistributingï¼šstreamçš„åˆ†åŒºå‘ç”Ÿæ”¹å˜ï¼Œæ•°æ®ä¼šä¾æ®é€»è¾‘ï¼Œå‘é€åˆ°ä¸åŒçš„ç›®æ ‡ä»»åŠ¡ä¸­å»

#### ç®—å­é“¾

Flinkçš„ä»»åŠ¡é“¾ä¼˜åŒ–æŠ€æœ¯ï¼Œåœ¨å¹¶è¡Œåº¦ç›¸åŒã€one-to-oneæ—¶å¯¹ä»»åŠ¡é“¾è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå‡å°‘æœ¬åœ°é€šä¿¡çš„å¼€é”€ï¼Œä»¥æœ¬åœ°è½¬å‘(local forward)çš„æ–¹å¼è¿›è¡Œè¿æ¥

#### æ‰§è¡Œå›¾

Flinkä¸­çš„æ‰§è¡Œå›¾åˆ†ä¸ºå››å±‚ï¼šStreamGraph -> JobGraph -> ExecutionGraph -> ç‰©ç†æ‰§è¡Œå›¾

* StreamGraph: ä¾æ®ä»£ç é€»è¾‘APIç”Ÿæˆçš„å›¾ï¼Œç”¨æ¥è¡¨ç¤ºç¨‹åºçš„æ‹“æ‰‘ç»“æ„
* JobGraph: ä¼˜åŒ–åçš„StreamGraphï¼Œå°†è¦æäº¤ç»™JobManagerçš„æ•°æ®ç»“æ„ã€‚ä¸»è¦å°†ç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹chainåœ¨ä¸€èµ·è¿›è¡Œä¼˜åŒ–
* ExecutionGraph: JobManagerç”Ÿæˆçš„Graphï¼ŒExecutionGraphæ˜¯JobGraphçš„å¹¶è¡ŒåŒ–ç‰ˆæœ¬ï¼Œæ˜¯è°ƒåº¦å±‚æœ€æ ¸å¿ƒçš„æ•°æ®ç»“æ„
* ç‰©ç†æ‰§è¡Œå›¾: JobManageræ ¹æ®ExecutionGraphè°ƒåº¦åï¼Œåœ¨TaskManagersä¸Šéƒ¨ç½²Taskåçš„å›¾ï¼Œä¸æ˜¯å…·ä½“çš„æ•°æ®ç»“æ„

![ExecutionGraphæ‰§è¡Œå›¾](ExecutionGraphæ‰§è¡Œå›¾.png)

#### Task \ TaskSlots

åœ¨Flinkä¸­ï¼Œä¸€ä¸ªTaskManager -> ä¸€ä¸ªJVMè¿›ç¨‹ -> nä¸ªTaskSlot -> mä¸ªTaskä»»åŠ¡ 

##### ä»»åŠ¡å…±äº«Slot

ç”±äºTaskSlotçš„èµ„æºç›¸ç­‰ï¼ŒTaskçš„è´Ÿè½½ä¸å‡ï¼Œè‹¥ä¸€ä¸ªTaskå ç”¨ä¸€ä¸ªTaskSlotï¼Œé‚£ä¹ˆé›†ç¾¤èµ„æºåˆ©ç”¨ç‡åˆ™ä¸é«˜ï¼Œæ‰€ä»¥Flinké»˜è®¤å¯ä»¥ä½¿ä¸åŒçš„Taskè¿è¡Œåœ¨åŒä¸€ä¸ªTaskSlotä¸­ï¼Œå«åšä»»åŠ¡å…±äº«Slot

è¿™æ ·å¯ä»¥ä½¿ä¸€ä¸ªslotè¿è¡ŒJobçš„æ•´ä¸ªç®¡é“ï¼Œè¿™æ ·`èµ„æºå¯†é›†å‹`å’Œ`éå¯†é›†å‹`çš„ä»»åŠ¡åŒæ—¶æ”¾åœ¨ä¸€ä¸ªslotä¸­å¯ä»¥è‡ªè¡Œè¯„åˆ†èµ„æºå ç”¨æ¯”ä¾‹ï¼Œå¹³è¡¡TaskManagerçš„è´Ÿè½½

1. Task Slot
	* é™æ€æ¦‚å¿µï¼ŒTaskManagerå…·æœ‰çš„å¹¶å‘æ‰§è¡Œèƒ½åŠ›
	* é€šè¿‡å‚æ•°taskmanager.numberOfTaskSlotsè¿›è¡Œé…ç½®
2. å¹¶è¡Œåº¦ (parallelism)
	* åŠ¨æ€æ¦‚å¿µï¼Œä¹Ÿå°±æ˜¯TaskManagerè¿è¡Œæ—¶å®é™…çš„å¹¶å‘èƒ½åŠ›
	* é€šè¿‡å‚æ•°parallelism.defaultè¿›è¡Œé…ç½®

è®¾ç½®å¹¶è¡Œåº¦çš„ä¸‰ç§å¸¸è§æ–¹å¼ï¼š

1. flink-conf.yaml: - yaml

	```yaml
	parallelism.default: 2
	```

2. Flink å®¢æˆ·ç«¯ - shell

	```shell
	bin/flink run -p 2
	```

3. ä»£ç æ‰§è¡Œç¯å¢ƒ - java

	```java
	env.setParallelism(2)
	```

4. ç®—å­æ‰§è¡Œå•ç‹¬ä¿®æ”¹ - java

	```java
	Operator().setParallelism(2)
	```

#### Other - å…³é—­ä¼˜åŒ–

å…³é—­é»˜è®¤ä¼˜åŒ– - chain

```java
// å…³é—­å‰åæ‰§è¡Œé“¾ä¼˜åŒ–
Operator().disableChaining();
env.disableChaining();

// å¼€å¯å½“å‰ç®—å­å‰åæ‰§è¡Œé“¾ä¼˜åŒ–
Operator().startNewChain();
env.startNewChain();
```

å…³é—­é»˜è®¤ä¼˜åŒ– - sharing slot

```java
// è®¾ç½®æŸç®—å­çš„slotå…±äº«ç»„
Operator().slotSharingGroup(String str)
// 1. åŒç»„çš„ç®—å­ï¼Œæ‰å¯ä»¥å…±äº«slot
// 2. è°ƒç”¨äº†slotSharingGroup()åï¼Œè¯¥ç®—å­åé¢çš„æ‰€æœ‰ç®—å­éƒ½æ˜¯è¯¥ç»„
```

## DataStream Api

Flinkçš„åŸºæœ¬æ­¥éª¤

1. è·å–æ‰§è¡Œç¯å¢ƒ(execution environment)
2. è¯»å–æ•°æ®æº(source)
3. å®šä¹‰åŸºäºæ•°æ®çš„è½¬æ¢æ“ä½œ(transformations)
4. å®šä¹‰è®¡ç®—ç»“æœçš„è¾“å‡ºä½ç½®(sink)
5. è§¦å‘ç¨‹åºæ‰§è¡Œ(execute)

![image-20230403213813630](png/zczntl-1.png)

### åˆ›å»ºæ‰§è¡Œç¯å¢ƒ

1. åˆ›å»ºæ‰¹å¼æ‰§è¡Œç¯å¢ƒ

	```java
	ExecutionEnvironment env =
	ExecutionEnvironment.getExecutionEnvironment()
	```

2. åˆ›å»ºæµå¼æ‰§è¡Œç¯å¢ƒ

	```java
	// è‡ªåŠ¨æ ¹æ®ä¸Šä¸‹æ–‡è·å–æ‰§è¡Œç¯å¢ƒï¼Œæœ¬æœºæ‰§è¡Œåˆ™æ˜¯æœ¬åœ°ï¼Œè¿è¡Œflinkç¨‹åºåˆ™æ˜¯è¿œç¨‹é›†ç¾¤æœåŠ¡
	StreamExecutionEnvironment env =
	StreamExecutionEnvironment.getExecutionEnvironment()
	    
	// åˆ›å»ºæœ¬åœ°æ‰§è¡Œç¯å¢ƒ
	StreamExecutionEnvironment localEnv =
	StreamExecutionEnvironment.createLocalEnvironment()
	    
	// åˆ›å»ºè¿œç¨‹é›†ç¾¤æ‰§è¡Œç¯å¢ƒ
	StreamExecutionEnvironment env =
	StreamExecutionEnvironment.createRemoteEnvironment(
	    hostname: "host",
	    port: 1234,
	    jar: "path/.jar"
	)
	```

3. ä»¥æµå¼Apiè¿è¡Œæ‰¹å¤„ç†
	DataStreamçš„ä¸‰ç§æ‰§è¡Œæ¨¡å¼

	* streaming: 

	* batch: 

	* automatic: 

	1. ä»¥javaä»£ç æŒ‡å®šï¼ˆä»£ç å†…ç½®å†™æ­»ï¼Œä¸æ¨èï¼‰

	```java
	env.setRuntimeMode(RuntimeExecutionMode.BATCH);
	```

	2. å‘½ä»¤è¡Œæäº¤ä»£ç æ—¶ï¼Œè¿›è¡Œé…ç½®

	```shell
	bin/flink run -Dexecution.runtime-mode=BATCH ...
	```

	> è™½ç„¶BATCHæ¨¡å¼çœ‹èµ·æ¥ä¼¼ä¹è¢«Streamingå…¨è¦†ç›–äº†
	>
	> åŒºåˆ«æ˜¯BATCHè¿è¡Œæ—¶ï¼Œæ•°æ®å…¨éƒ¨å¤„ç†å®Œï¼Œæ‰ä¼šä¸€æ¬¡æ€§è¾“å‡ºç»“æœ
	>
	> ä½†æ˜¯Streamè¿è¡Œä¸­ï¼Œæµå¼å¤„ç†æ¨¡å¼å°†ä¼šäº§ç”Ÿæ›´å¤šçš„ä¸­é—´ç»“æœè¾“å‡º
	>
	> æ‰€ä»¥åœ¨æœ¬æ¥è¾“å…¥æœ‰ç•Œã€åªå¸Œæœ›é€šè¿‡æ‰¹å¤„ç†å¾—åˆ°æœ€ç»ˆç»“æœçš„åœºæ™¯ä¸‹ï¼ŒStreamingæ¨¡å¼ä¸‹ä¼šä¸å¤Ÿé«˜æ•ˆ

### DataSource

#### readTextFile / socketTextStream

```java
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
// env.setParallelism(1);
DataStreamSource<String> textFile = env.readTextFile("input/UserEvent.log");
// DataStreamSource<String> socketTextStream = env.socketTextStream("node1", 17788);

// å°è£…å¤„ç†é€»è¾‘
SingleOutputStreamOperator<Event> map = textFile
    .map(s -> new Event(s.split(",")[0], s.split(",")[1], Long.valueOf(s.split(",")[2])));

map.print();
env.execute();
```

#### KafkaSource

<1.15.3å®˜ç½‘æ›´æ–°ç‰ˆæœ¬>

1. å¯¼å…¥flinkè¿æ¥kafkaçš„ä¾èµ–

	```xml
	<dependency>
	    <groupId>org.apache.flink</groupId>
	    <artifactId>flink-connector-kafka</artifactId>
	    <version>${flink.veresion}</version>
	</dependency>
	```

2. é…ç½®kafkaSourceï¼Œè¯»å–æ•°æ®

	```java
	/* è¯»å–kafkaå†…å®¹ */
	KafkaSource<String> source = KafkaSource.<String>builder()
	    .setBootstrapServers("node1:9092")
	    .setTopics("input-topic")
	    .setGroupId("my-group")
	    .setStartingOffsets(OffsetsInitializer.earliest())
	    .setValueOnlyDeserializer(new SimpleStringSchema())
	    .build();
	
	DataStreamSource<String> kafkaSource = 
	    env.fromSource(source, WatermarkStrategy.noWatermarks(), "Kafka Source");
	```

#### DIY Source



1. ParallelSourceFunction

	```java
	StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
	DataStreamSource<Event> source = env.addSource(new SourceFunction<Event>() {
	    private Boolean running = true;
	    @Override
	    public void run(SourceContext<Event> sourceContext) {
	        Random random = new Random();
	        String[] users = {"Mary", "Lily", "Bob", "Alix"};
	        String[] urls = {"./home", "./math", "./product?id=2232"};
	        while (running) {
	            sourceContext.collect(new Event(
	                users[random.nextInt(users.length)],
	                urls[random.nextInt(urls.length)],
	                new Date().getTime()
	            ));
	        }
	    }
	
	    @Override
	    public void cancel() {
	        running = false;
	    }
	});
	
	source.print();
	env.execute();
	```

2. ParallelSourceFunction

	```java
	StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
	env.setParallelism(4);
	// ä¿®æ”¹class - SourceFunction ä¸º ParallelSourceFunction
	DataStreamSource<Event> source = env.addSource(new ParallelSourceFunction<Event>() {
			...
	
	    @Override
	    public void run(SourceContext<Event> sourceContext) {
			...
	    }
	
	    @Override
	    public void cancel() {
			...
	    }
	}).setParallelism(2); // å¯ä»¥è®¾ç½®å¹¶è¡Œåº¦ï¼Œä½¿å…¶å¹¶è¡Œè¯»å–
	
	source.print();
	env.execute();
	```

### SingleOutputStreamOperator

#### BasicTransformation

##### Map

è¿›è¡Œ`æ˜ å°„`æ“ä½œï¼Œå¸¸ç”¨äºè½¬æ¢ç±»å‹ï¼Œæˆ–è€…è·å–åˆ°æ•°æ®ä¸­éœ€è¦çš„å†…å®¹ä½¿ç”¨

> public <R> SingleOutputStreamOperator<R> map(MapFunction<T, R> mapper)

  ```java
  SingleOutputStreamOperator<Event> map = textFile.map(
      s -> new Event(s.split(",")[0], s.split(",")[1], Long.valueOf(s.split(",")[2]))
  ).returns(Types.POJO(Event.class));
  ```

##### Filter

è¿›è¡Œè¿‡æ»¤æ“ä½œï¼Œè¿”å›trueçš„å…ƒç´ æ­£å¸¸è¾“å‡ºï¼Œfalseå…ƒç´ ä¼šè¢«æ‹¦æˆª

> public SingleOutputStreamOperator<T> filter(FilterFunction<T> filter)

```java
map = map.filter(value -> !"Bob".equals(value.getName()));
```

##### FlatMap

è¿›è¡Œ`æ‰å¹³æ˜ å°„`æ“ä½œï¼Œè¿”å›åˆ—è¡¨å†…å®¹ä¸­çš„æ‰€æœ‰å…ƒç´ 

> public <R> SingleOutputStreamOperator<R> flatMap(FlatMapFunction<T, R> flatMapper)

```java
SingleOutputStreamOperator<String> map = wordTxt.flatMap(
    (String value, Collector<String> out) -> Arrays.stream(value.split(" ")).forEach(out::collect)
).returns(Types.STRING);
```

#### AggregationTransformation

##### keyBy & reduce

æ ¹æ®ç»™å®šé€»è¾‘åˆ†åˆ«è¿›è¡Œåˆ†ç»„èšåˆï¼Œå±äºæ˜¯é€šç”¨é€»è¾‘ï¼Œä½†æ˜¯å®ç°å¤æ‚

> public <K> KeyedStream<T, K> keyBy(KeySelector<T, K> key)
>
> & public SingleOutputStreamOperator<T> reduce(ReduceFunction<T> reducer)

```java
SingleOutputStreamOperator<Tuple2<String, Long>> tuple2KeyValue = source.flatMap(
    (String value, Collector<Tuple2<String, Long>> out) ->
    Arrays.stream(value.split(" ")).forEach(
        word -> out.collect(new Tuple2<>(word, 1L))
    )
).returns(Types.TUPLE(Types.STRING, Types.LONG));

KeyedStream<Tuple2<String, Long>, String> tuple2StringKeyedStream = tuple2KeyValue.keyBy(value -> value.f0);

SingleOutputStreamOperator<Tuple2<String, Long>> keyed = tuple2StringKeyedStream.reduce(
    (Tuple2<String, Long> v1, Tuple2<String, Long> v2) -> Tuple2.of(v1.f0, v1.f1 + v2.f1)
).returns(Types.TUPLE(Types.STRING, Types.LONG));

keyed.keyBy(value -> "default").reduce(
    (Tuple2<String, Long> red1, Tuple2<String, Long> red2) -> red1.f1 > red2.f1 ? red1 : red2
).returns(Types.TUPLE(Types.STRING, Types.LONG)).print();
env.execute();
```

##### keyBy & max/maxBy

é€šè¿‡KeyByåˆ†ç»„ï¼Œæ±‚æœ€å¤§å€¼ï¼Œ

maxï¼šä¼šæ‰¾å‡ºä»¥maxä¸ºæœ€å¤§å€¼çš„maxï¼Œå…¶ä»–æ•°æ®ä¸ºåˆå§‹æ•°æ®

maxByï¼šä¼šæ‰¾å‡ºä»¥maxä¸ºæœ€å¤§å€¼çš„æ•°æ®

> public SingleOutputStreamOperator<T> max(int positionToMax)
>
> & public SingleOutputStreamOperator<T> maxBy(int positionToMaxBy)

```java
// hadoop hadoop hadoop
// shujie shujie shujie shujie
keyed.keyBy(value -> "default").max(1).returns(Types.TUPLE(Types.STRING, Types.LONG)).print();
// hadoop 4

keyed.keyBy(value -> "default").maxBy(1).returns(Types.TUPLE(Types.STRING, Types.LONG)).print();
// shujie 4
```

#### RichFunction

å¯Œå‡½æ•°ï¼šæä¾›æ¯”å¸¸è§„å‡½æ•°æ›´å¤šçš„ï¼Œæ›´ä¸°å¯ŒåŠŸèƒ½çš„ç±»ï¼Œå¯ä»¥è·å–åˆ°`è¿è¡Œç¯å¢ƒçš„ä¸Šä¸‹æ–‡`ï¼Œæ‹¥æœ‰ä¸€äº›`ç”Ÿå‘½å‘¨æœŸæ–¹æ³•`ç­‰ï¼Œå¯ä»¥å®ç°æ›´ä¸°å¯Œçš„åŠŸèƒ½ã€‚

#### ç‰©ç†åˆ†åŒº <Physical Partitioning>

åˆ†åŒºç­–ç•¥ï¼š

1. shuffleï¼šä»¥`éšæœºç­–ç•¥`å°†è‹¥å¹²æ•°æ®ï¼Œå‘é€åˆ°æ‰€æœ‰çš„åˆ†åŒºä¸­å»

	<img src="png/Flinkåˆ†åŒºShuffleç­–ç•¥.png" alt="Flinkåˆ†åŒºShuffleç­–ç•¥" style="zoom:150%;" />

2. rebalanceï¼šä»¥`è½®è¯¢ç­–ç•¥`å°†è‹¥å¹²æ•°æ®ï¼Œå‘é€åˆ°æ‰€æœ‰çš„åˆ†åŒºä¸­å»

	<img src="png/Flinkåˆ†åŒºBalanceç­–ç•¥.png" alt="Flinkåˆ†åŒºBalanceç­–ç•¥" style="zoom:150%;" />

3. rescaleï¼šä»¥`TaskManagerå†…è½®è¯¢ç­–ç•¥`å°†è‹¥å¹²æ•°æ®ï¼Œå‘é€åˆ°å½“å‰çš„TaskManagerçš„åˆ†åŒºä¸­å»

	<img src="png/Flinkåˆ†åŒºScaleç­–ç•¥.png" alt="Flinkåˆ†åŒºScaleç­–ç•¥" style="zoom:150%;" />

ç‰¹æ®Šåˆ†åŒºï¼š

1. broadcastï¼šä»¥`å¹¿æ’­ç­–ç•¥`å°†è‹¥å¹²æ•°æ®ï¼Œå‘é€åˆ°æ‰€æœ‰çš„åˆ†åŒºä¸­å»

	<img src="png/Flinkåˆ†åŒºBroadcastç­–ç•¥.png" alt="Flinkåˆ†åŒºBroadcastç­–ç•¥" style="zoom:150%;" />

2. globalï¼šä»¥`å…¨å±€ç­–ç•¥`å°†è‹¥å¹²æ•°æ®ï¼Œå‘é€åˆ°ä¸€ä¸ªåˆ†åŒºä¸­å»

	<img src="png/Flinkåˆ†åŒºGlobalç­–ç•¥.png" alt="Flinkåˆ†åŒºGlobalç­–ç•¥" style="zoom:150%;" />

###  Sink

#### LocalFileSystemSink

```java
SingleOutputStreamOperator<String> returns = ... ;

StreamingFileSink<String> sink = StreamingFileSink.<String>forRowFormat(
    	new Path("src/main/resources/output"),		// é…ç½®æ–‡ä»¶è¾“å‡ºè·¯å¾„
    	new SimpleStringEncoder<>("UTF-8")			// é…ç½®ç¼–ç æ ¼å¼
)											// è¿”å›sink,ä½¿ç”¨é»˜è®¤çš„æ»šåŠ¨ç­–ç•¥
    .withRollingPolicy(
    	DefaultRollingPolicy.builder()
	    .withMaxPartSize(MemorySize.ofMebiBytes(10L)) //ç¼“å­˜10Mbyts
	    .withRolloverInterval(Duration.ofSeconds(60)) // 60sç”Ÿæˆä¸€ä¸ªæ–°æ–‡ä»¶
	    .withInactivityInterval(Duration.ofSeconds(5)) // å¦‚æœæ— äº‹ä»¶è§¦å‘
	    .build()
	).build();

returns.addSink(sink); // sink
```

#### KafkaSink

```xml
<!-- flink è¿æ¥ kafka çš„åŒ… -->
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-kafka</artifactId>
    <version>${flink.veresion}</version>
</dependency>
```

```java
SingleOutputStreamOperator<String> returns = ... ;

KafkaSink<String> sink = KafkaSink.<String>builder()
    .setBootstrapServers("node1:9092")	// kafkaé›†ç¾¤åœ°å€
    .setRecordSerializer(
    	KafkaRecordSerializationSchema
    		.builder()
    		.setTopic("output-topic")	// kafka topic
    		.setValueSerializationSchema(new SimpleStringSchema())
    		.build()
	)
    .setDeliverGuarantee(DeliveryGuarantee.AT_LEAST_ONCE) 	// è®¾ç½®è‡³å°‘æ¶ˆè´¹ä¸€æ¬¡
    .build();

returns.sinkTo(sink);
```

#### RedisSink

```xml
<!-- https://mvnrepository.com/artifact/org.apache.bahir/flink-connector-redis -->
<dependency>
    <groupId>org.apache.bahir</groupId>
    <artifactId>flink-connector-redis_2.11</artifactId>
    <version>1.1-SNAPSHOT</version>
</dependency>
```

```java
SingleOutputStreamOperator<Event> returns = ... ;

HashSet<InetSocketAddress> inetSocketAddressHashSet = new HashSet<>();
inetSocketAddressHashSet.add(new InetSocketAddress("node1", 7000));	// èŠ‚ç‚¹1ä¿¡æ¯
inetSocketAddressHashSet.add(new InetSocketAddress("node2", 7000)); // èŠ‚ç‚¹2ä¿¡æ¯
inetSocketAddressHashSet.add(new InetSocketAddress("node3", 7000)); // èŠ‚ç‚¹3ä¿¡æ¯

source.addSink(new RedisSink<>(
    // å¦‚æœæ˜¯åˆ†ç‰‡é›†ç¾¤çš„è¯ä½¿ç”¨é›†ç¾¤æ¨¡å¼çš„ï¼ˆFlinkJedisClusterConfigç±»ï¼‰
    // å¦‚æœæ˜¯å“¨å…µé›†ç¾¤çš„è¯ä½¿ç”¨å“¨å…µæ¨¡å¼çš„ï¼ˆFlinkJedisSentinelConfigç±»ï¼‰
    // å¦‚æœæ˜¯å•ç‚¹è®¿é—®çš„è¯ä½¿ç”¨ç‹¬ç«‹æ¨¡å¼çš„ï¼ˆFlinkJedisPoolConfigç±»ï¼‰
    new FlinkJedisClusterConfig.Builder().setNodes(inetSocketAddressHashSet).build(),
    new RedisMapper<Event>() {
        @Override
        public RedisCommandDescription getCommandDescription() {
            return new RedisCommandDescription(RedisCommand.HSET, "HASH_NAME", 3);
            // ç¼–å†™redisé…ç½®ä¿¡æ¯ï¼Œä½¿ç”¨ä»€ä¹ˆæ–¹å¼ï¼Œè¿‡æœŸäº‹ä»¶ï¼Œsetæ–¹å¼ç­‰
        }

        @Override
        public String getKeyFromData(Event event) {	 // ä»è¾“å…¥æ•°æ®ä¸­è·å–key
            return event.getName();
        }

        @Override
        public String getValueFromData(Event event) {	// ä»è¾“å…¥æ•°æ®ä¸­è·å–valueå€¼
            return event.toString();
        }
    }
));
```

#### ElasticsearchSink

```xml
<!-- flink è¿æ¥ elasticsearchçš„åŒ… -->
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-elasticsearch7</artifactId>
    <version>${flink.veresion}</version>
</dependency>
```

```java
// function
private static IndexRequest createIndexRequest(String element) {
    Map<String, Object> json = new HashMap<>();
    json.put("data", element);

    return Requests.indexRequest()
        .index("my-index")
        .id(element)
        .source(json);
}
```

```java
SingleOutputStreamOperator<String> returns = ... ;
returns.sinkTo(new Elasticsearch7SinkBuilder<String>()
               // å•æ¡æ•°æ®ä¸ºå•ä½æ’å…¥æ•°ï¼Œç¼“å­˜æ•°é‡
               .setBulkFlushMaxActions(1) 
               // è®¾ç½®esé›†ç¾¤åœ°å€
               .setHosts(
                   new HttpHost("192.168.32.151", 9200, "http"),
                   new HttpHost("192.168.32.152", 9200, "http"),
                   new HttpHost("192.168.32.153", 9200, "http"))
               // è®¾ç½®åˆ›å»ºç´¢å¼•ï¼Œä»¥åŠå…ƒç´ çš„idï¼Œsourceç­‰æ•°æ®æ¥æº
               .setEmitter(
                   (element, context, indexer) -> indexer.add(createIndexRequest(element)))
               .build()
              );

env.execute();
```

#### MysqlSink

å¯¼å…¥Flink jdbcè¿æ¥ï¼Œmysqlé©±åŠ¨jaråŒ…

```xml
 <!-- flink è¿æ¥ jdbcçš„åŒ… -->
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-jdbc</artifactId>
    <version>${flink.veresion}</version>
</dependency>
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>5.1.47</version>
</dependency>
```

```java
DataStreamSource<Event> source = env.addSource(new DiyParallelSourceFunc()).setParallelism(2);


SinkFunction<Event> sink = JdbcSink.sink(
    "insert into events (name, uri, time) values (?, ?, ?)",                       // mandatory
    (PreparedStatement preparedStatement, Event event) -> {
        preparedStatement.setString(1, event.getName());
        preparedStatement.setString(2, event.getName());
        preparedStatement.setLong(3, event.getTime());
    },// mandatory
    JdbcExecutionOptions.builder()
    .withBatchIntervalMs(200)             // optional: default = 0, meaning no time-based execution is done
    .withBatchSize(1000)                  // optional: default = 5000 values
    .withMaxRetries(5)                    // optional: default = 3
    .build(),                  // optional
    new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
    .withUrl("jdbc:mysql://node1:3306/flinkSink?useSSL=false")
    .withUsername("root")
    .withPassword("shujie")
    .withDriverName("com.mysql.jdbc.Driver")
    .build()                  // mandatory
);

source.addSink(sink);
```

#### DiySink model

```java
package top.taurushu.streamSink;

import org.apache.flink.api.common.eventtime.Watermark;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;
import top.taurushu.streamSource.DiyParallelSourceFunc;
import top.taurushu.streamSource.Event;

public class WriteDiySink {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource<Event> source = env.addSource(new DiyParallelSourceFunc()).setParallelism(2);


        source.addSink(new MySinkFunction());

        env.execute();
    }
}

class MySinkFunction extends RichSinkFunction<Event> {

    @Override
    public void open(Configuration parameters) throws Exception {
        super.open(parameters);
    }

    @Override
    public void invoke(Event value, Context context) throws Exception {
        super.invoke(value, context);
    }

    @Override
    public void writeWatermark(Watermark watermark) throws Exception {
        super.writeWatermark(watermark);
    }

    @Override
    public void finish() throws Exception {
        super.finish();
    }

    @Override
    public void close() throws Exception {
        super.close();
    }
}
```

## æ—¶é—´/æ°´å°/çª—å£

### Time(æ—¶é—´)

1. å¤„ç†æ—¶é—´(Processing Time)
		å¤„ç†æ—¶é—´ï¼ŒæŒ‡æ‰§è¡Œå¤„ç†æ“ä½œçš„æœºå™¨çš„`ç³»ç»Ÿæ—¶é—´`ã€‚

2. äº‹ä»¶æ—¶é—´(Event Time)
		äº‹ä»¶æ—¶é—´ï¼ŒæŒ‡äº‹ä»¶å‘ç”Ÿçš„æ—¶é—´ï¼Œä¹Ÿå°±æ˜¯`æ•°æ®ç”Ÿæˆçš„æ—¶é—´`ã€‚æ˜¯æ•°æ®çš„å±æ€§ï¼Œä¹Ÿå°±æ˜¯è¿™æ¡æ•°æ®è®°å½•çš„"æ—¶é—´æˆ³ Timestamp"

	â€‹	åœ¨äº‹ä»¶æ—¶é—´è¯­ä¹‰ä¸‹ï¼Œæˆ‘ä»¬å¯¹äºæ—¶é—´çš„è¡¡é‡ï¼Œå°±ä¸çœ‹ä»»ä½•æœºå™¨çš„ç³»ç»Ÿæ—¶é—´äº†ï¼Œè€Œæ˜¯ä¾èµ–äºæ•°æ®æœ¬èº«ã€‚ç”±äºåˆ†å¸ƒå¼ç³»ç»Ÿä¸­ç½‘ç»œ`ä¼ è¾“å»¶è¿Ÿ`çš„ä¸ç¡®å®šæ€§ï¼Œå®é™…åº”ç”¨ä¸­æˆ‘ä»¬è¦é¢å¯¹çš„æ•°æ®æµå¾€å¾€æ˜¯`ä¹±åº`çš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå°±ä¸èƒ½ç®€å•åœ°æŠŠæ•°æ®è‡ªå¸¦çš„æ—¶é—´æˆ³å½“ä½œæ—¶é’Ÿäº†ï¼Œè€Œéœ€è¦ç”¨å¦å¤–çš„æ ‡å¿—æ¥è¡¨ç¤ºäº‹ä»¶æ—¶é—´è¿›å±•ï¼Œåœ¨Flinkä¸­æŠŠå®ƒå«ä½œäº‹ä»¶æ—¶é—´çš„ "æ°´ä½çº¿ `Watermark`"

> Flink1.12ç‰ˆæœ¬åï¼Œé»˜è®¤çš„æ—¶é—´è®¾ç½®çš„æ˜¯`äº‹ä»¶æ—¶é—´Event Time`

### WaterMark(æ°´ä½çº¿/æ°´å°)

æ°´ä½çº¿çš„å«ä¹‰ï¼šåœ¨æ°´ä½çº¿ä¹‹åï¼Œä¸ä¼šå†å‡ºç°æ°´ä½çº¿ä¹‹å‰çš„æ—¶é—´(EventTime)ï¼Œä»£è¡¨æ°´ä½çº¿æ—¶ï¼Œæ•°æ®å·²ç»åˆ°é½äº†

#### ä¹±åºç­–ç•¥

ä½¿ç”¨`Flinkå†…ç½®ç­–ç•¥ - forBoundedOutOfOrderness with TimestampAssigner`ï¼Œ

åœ¨ä»£ç ä¸­ç”Ÿæˆ`ä¹±åºæœ‰ç•Œçš„æ ¹æ®ç»™å®šæ—¶é—´çš„`æ°´ä½çº¿ï¼š

```java
DataStream<Event> source = ... ;

SingleOutputStreamOperator<Event> kfkSrcWithWM2 =
        source.assignTimestampsAndWatermarks(
                WatermarkStrategy
                        .<Event>forBoundedOutOfOrderness(Duration.ofMillis(120))
                        .withTimestampAssigner(
                                (SerializableTimestampAssigner<Event>)
                                        (element, recordTimestamp) -> element.getTime()
                        )
        );
```

#### æœ‰åºç­–ç•¥

ä½¿ç”¨`Flinkå†…ç½®ç­–ç•¥ - forMonotonousTimestamps with TimestampAssigner`ï¼Œ

åœ¨ä»£ç ä¸­ç”Ÿæˆ`å•è°ƒæ—¶é—´`çš„æ°´ä½çº¿ï¼š

```java
DataStream<Event> source = ... ;

SingleOutputStreamOperator<Event> kfkSrcWithWM2 =
        source.assignTimestampsAndWatermarks(
                WatermarkStrategy
                        .<Event>forMonotonousTimestamps()  // ä¿®æ”¹ä¸ºå•è°ƒçš„æ—¶é—´
                        .withTimestampAssigner(
                                (SerializableTimestampAssigner<Event>)
                                        (element, recordTimestamp) -> element.getTime()
                        )
        );
```

#### è‡ªå®šä¹‰æ°´ä½çº¿ç­–ç•¥

`åŸºäºäº‹ä»¶`å‘é€æ°´å°çš„

```java
(WatermarkStrategy<Event>) context -> new WatermarkGenerator<Event>() {
    @Override
    public void onEvent(Event event, long eventTimestamp, WatermarkOutput output) {
        output.emitWatermark(new Watermark(event.getTime()));
    }

    @Override
    public void onPeriodicEmit(WatermarkOutput output) {

    }
}
```

`åŸºäºå‘¨æœŸ`å‘é€`å…è®¸è¿Ÿåˆ°çš„`æ°´å°

```java
(WatermarkStrategy<Event>) context -> new WatermarkGenerator<Event>() {
    private long maxTimestamp;

    @Override
    public void onEvent(Event event, long eventTimestamp, WatermarkOutput output) {
        maxTimestamp = Math.max(maxTimestamp, event.getTime());
    }

    @Override
    public void onPeriodicEmit(WatermarkOutput output) {
        output.emitWatermark(new Watermark(maxTimestamp - 120 - 1));
    }
}
```

### Window(çª—å£)

#### æŒ‰ç…§é©±åŠ¨ç±»å‹åˆ’åˆ†

<img src="png/é©±åŠ¨ç±»å‹åˆ†ç±»çª—å£.png" alt="é©±åŠ¨ç±»å‹åˆ†ç±»çª—å£" style="zoom:150%;" />

##### 1. TimeWindow æ—¶é—´çª—å£

ä»¥äº‹æ—¶é—´ç‚¹æ¥å®šä¹‰çª—å£çš„å¼€å§‹ä¸ç»“æŸï¼Œåˆ°è¾¾ç»“æŸæ—¶é—´åï¼Œçª—å£ä¸åœ¨æ”¶é›†æ•°æ®

##### 2. CountWindow è®¡æ•°çª—å£

ä»¥çª—å£çš„æ•°æ®ä¸ªæ•°ä½œä¸ºå¼€å§‹å’Œç»“æŸï¼Œåªéœ€è¦é™åˆ¶çª—å£å¤§å°ï¼Œå°±å¯ä»¥æŠŠæ•°æ®åˆ†é…åˆ°å¯¹åº”çš„çª—å£ä¸­

#### æŒ‰ç…§çª—å£åˆ†é…æ•°æ®çš„è§„åˆ™åˆ†ç±»

##### 1. æ»šåŠ¨çª—å£(Tumbling Window)

å¯¹åº”ç±» ğŸ‘‰[`Tumbling[EventTime/ProcessingTime]Windows`](#TumblingWindows)

æ»šåŠ¨çª—å£ç‰¹ç‚¹ï¼šçª—å£`å¤§å°å›ºå®š`ï¼Œå°†æ•°æ®`å‡åŒ€åˆ‡ç‰‡`ï¼Œçª—å£é—´`æ²¡æœ‰é‡å `ï¼Œ`ä¸€ä¸ª`æ•°æ®åˆ†é…å±äº`ä¸€ä¸ª`çª—å£

<img src="png/sv2kl9-1.png" alt="image-20230407174524482" style="zoom:67%;" />

##### 2. æ»‘åŠ¨çª—å£(Sliding Window)

å¯¹åº”ç±» ğŸ‘‰[`Sliding[EventTime/ProcessingTime]Windows`](#SlidingWindows)

æ»‘åŠ¨çª—å£ç‰¹ç‚¹ï¼šçª—å£`å¤§å°å›ºå®š`ï¼Œçª—å£é—´`å­˜åœ¨æ»‘åŠ¨æ­¥é•¿`ï¼Œçª—å£é—´`æœ‰`é‡å éƒ¨åˆ†ï¼Œ`ä¸€ä¸ª`æ•°æ®å¯èƒ½å±äº`å¤šä¸ª`çª—å£

<img src="png/szuu9a-1.png" alt="image-20230407175327973" style="zoom: 67%;" />

##### 3. ä¼šè¯çª—å£(Session Window)

å¯¹åº”ç±»[`[Processing/DynamicProcessing/DynamicEvent/Event]TimeSessionWindow`](#SessionWindow)

ä¼šè¯çª—å£ï¼šçª—å£å¤§å°`ä¸å›ºå®š`ï¼ŒæŒ‰ç…§ä¼šè¯`é—´éš”å¤§å°`ï¼Œè¿›è¡Œåˆ†å‰²çª—å£ï¼Œçª—å£é—´æœ‰é—´éš”ä¸”é—´éš”`å¤§äº`è®¾å®šçš„é—´éš”å¤§å°

<img src="png//x70gpm-1.png" alt="image-20230407200640168" style="zoom:67%;" />

##### 4. å…¨å±€çª—å£(Global Window)

å¯¹åº”ç±»[`GlobalWindows`]

å…¨å±€çª—å£ï¼šæŠŠç›¸åŒkeyçš„`æ‰€æœ‰æ•°æ®`éƒ½åˆ†é…åˆ°`ä¸€ä¸ªçª—å£`ä¸­ï¼Œé»˜è®¤ä¸ä¼šè§¦å‘è®¡ç®—ï¼Œéœ€è¦`è‡ªå®šä¹‰è§¦å‘å™¨`ï¼Œè¿›è¡Œè®¡ç®—å¤„ç†

<img src="png/xczzkl-1.png" alt="image-20230407201721620" style="zoom:67%;" />

### çª—å£åˆ†ç±»

#### æŒ‰é”®åˆ†åŒº Keyed

åŸºäºkeyByåå¯¹`KeyedStream<T, K>`å»ºç«‹çª—å£

```java
stream.keyBy(element -> element.getKey()).window(WindowAssigner<?> assigner).xxx()
```

#### æŒ‰é”®åˆ†åŒº Non-Keyed

åŸºäºDataStreamå¯¹`å…¨éƒ¨`æ•°æ®å»ºç«‹çª—å£ [è¿™æ ·çš„è¯ï¼Œå…¨å±€å¹¶è¡Œåº¦åˆ™æ˜¯1ï¼Œå®é™…åº”ç”¨ä¸­ä¸æ¨èä½¿ç”¨]

```java
stream.windowAll(...).xxx()
```

### çª—å£API

æ€»çš„æ¥è¯´ï¼Œä¸€ä¸ªçª—å£å‡½æ•°åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼š1.çª—å£åˆ’åˆ†é€»è¾‘ [WindowAssigner] 2.çª—å£è®¡ç®—é€»è¾‘ [WindowFunction]

![Flinkæµä¹‹é—´çš„è½¬æ¢](png/Flinkæµä¹‹é—´çš„è½¬æ¢.png)

#### çª—å£åˆ’åˆ†é€»è¾‘ [WindowAssigner]

å®šä¹‰çª—å£ç±»å‹ï¼Œçª—å£å¦‚ä½•åˆ’åˆ†

1. <a name="TumblingWindows">æ»šåŠ¨çª—å£ Tumbling[EventTime/ProcessingTime]Windows</a>

	```java
	// éœ€è¦ä¼ ä¸€ä¸ªå‚æ•° - æ»šåŠ¨çª—å£çš„çª—å£å¤§å°
	stream.keyBy(key -> "default")
	    .window(TumblingEventTimeWindows.of(Time.milliseconds(1000)))
	    // å¤„ç†æ—¶é—´çª—å£/ä¼šè¯æ—¶é—´
		// .window(TumblingProcessingTimeWindows.of(Time.milliseconds(1000)))
	    .reduce((ReduceFunction<Long>) Long::sum)
	    .print();
	```

2. <a name="SlidingWindows">æ»‘åŠ¨çª—å£ Sliding[EventTime/ProcessingTime]Windows</a>

	```java
	// éœ€è¦ä¼ ä¸€ä¸ªå‚æ•° - åˆ’åŠ¨çª—å£çš„çª—å£å¤§å°ï¼Œä»¥åŠçª—å£ç§»åŠ¨çš„æ­¥é•¿ 
	stream.keyBy(key -> "default")
	    .window(SlidingEventTimeWindows.of(Time.milliseconds(1000),Time.milliseconds(500)))
		// å¤„ç†æ—¶é—´çª—å£/ä¼šè¯æ—¶é—´
		// .window(SlidingProcessingTimeWindows.of(Time.milliseconds(1000),Time.milliseconds(500)))
	    .reduce((ReduceFunction<Long>) Long::sum)
	    .print();
	```

3. <a name="SessionWindow">ä¼šè¯çª—å£ [Processing/DynamicProcessing/DynamicEvent/Event]TimeSessionWindow</a>

4. `è®¡æ•°çª—å£ countWindow`

	```java
	// éœ€è¦ä¼ ä¸€ä¸ªå‚æ•° - è®¡æ•°çª—å£ä¸­æ¯ä¸ªçª—å£çš„å…ƒç´ æ•°é‡
	stream.keyBy(key -> "default")
	    .countWindow(200)
	    .reduce((ReduceFunction<Long>) Long::sum)
	    .print();
	```

#### çª—å£è®¡ç®—é€»è¾‘ [WindowedStream / WindowFunction]

å…¸å‹çš„å¢é‡èšåˆå‡½æ•°æœ‰ï¼šReduceFunctionå½’çº¦å‡½æ•° / AggregateFunction

å¸¸è§çš„æœ‰ï¼šmin/minBy/max/maxBy/`aggregate`/apply(è€ç‰ˆæœ¬ï¼ŒåŠŸèƒ½è¢«processå–ä»£)/`process`/`reduce`

| å‡½æ•°                                                         |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| `reduce`(ReduceFunction(V, V))                               | å°†ä¸¤ä¸ªå…ƒç´ ï¼ŒæŒ‰ç…§`è‡ªå®šä¹‰é€»è¾‘`å¤„ç†ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„å…ƒç´            |
| `aggregate`(AggregateFunction<IN,ACC,OUT>)                   | æ ¹æ®ä¼ å…¥çš„æ³›å‹ï¼Œå®ç°çš„å››ä¸ªå‡½æ•°ï¼Œå®šä¹‰èšåˆå‡½æ•°                 |
| `aggregate`(AggregateFunction<IN,ACC,OUT>,<br/>ProcessWindowFunction<IN, OUT, KEY, W>) | ä¸¤ä¸ªå‡½æ•°ç›¸é…åˆä½¿ç”¨ï¼Œåœ¨aggregateå‡½æ•°ä¸­è®¡ç®—æ•°æ®ï¼Œåœ¨processå‡½æ•°å‘é€æ•°æ® |
| `process`(ProcessWindowFunction<IN, OUT, KEY, W>)            | æ ¹æ®ä¼ å…¥æ³›å‹å€¼åœ¨processå‡½æ•°ä¸­è¿›è¡Œè®¡ç®—ï¼Œå‘é€æ•°æ®              |

> * `reduce`:
> 	* ReduceFunction(V, V) -> V
> 		* Vï¼šè¿­ä»£è®¡ç®— value ç±»å‹ï¼Œè¿”å›valueç›¸åŒç±»å‹ï¼Œé€‚ç”¨äºç›¸åŒæ•°æ®é—´çš„è¿­ä»£è®¡ç®—
> * `aggregate`ï¼š
>   * AggregateFunction<IN, ACC, OUT> -> OUT
>   	* INï¼šè¾“å…¥æ•°æ®ç±»å‹
>   	* ACCï¼šç´¯åŠ å™¨ç±»å‹
>   	* OUTï¼šè¾“å‡ºæ•°æ®ç±»å‹
>   * æ–¹æ³•ï¼š
>   	* createAccumulateï¼šåˆå§‹åŒ–ç´¯åŠ å™¨
>   	* addï¼šaccumulatorï¼šè¿­ä»£è®¡ç®—
>   	* getResultï¼šè·å–è¿”å›å€¼
>   	* mergeï¼šçª—å£æ•°æ®åˆå¹¶ [`ä¼šè¯çª—å£`]
> * `process`ï¼š
> 	* ProcessWindowFunction<IN, OUT, KEY, W extends Window> -> OUT
> 		* INï¼šè¾“å…¥æ•°æ®ç±»å‹
> 		* OUTï¼šè¾“å‡ºæ•°æ®ç±»å‹
> 		* KEYï¼škeyByå­—æ®µåˆ†ç»„ç±»å‹
> 		* W extends Windowï¼šè®¾å®šçª—å£ç±»å‹
> * `aggregate`(AggregateFunction<T, ACC, V> aggFunction, 
> 	         ProcessWindowFunction<V, R, K, W> windowFunction)
> 	* AggregateFunction<T, ACC, V> -> V
> 		* Tï¼šè¾“å…¥çš„æ•°æ®ç±»å‹
> 		* ACCï¼šç´¯åŠ å™¨ç±»å‹
> 		* Vï¼šAggregateFunctionçš„æ•°æ®è¾“å‡ºç±»å‹å’ŒProcessWindowFunctionçš„æ•°æ®è¾“å…¥ç±»å‹
> 	* ProcessWindowFunction<V, R, K, W> -> R
> 		* Vï¼šAggregateFunctionçš„æ•°æ®è¾“å‡ºç±»å‹å’ŒProcessWindowFunctionçš„æ•°æ®è¾“å…¥ç±»å‹
> 		* Rï¼šè¿”å›æ•°æ®çš„ç±»å‹
> 		* Kï¼škeyByå­—æ®µç±»å‹
> 		* Wï¼šç»™å®šçª—å£ç±»å‹

```java
package top.taurushu.window;

import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.AggregateFunction;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;
import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.windows.TimeWindow;
import org.apache.flink.util.Collector;
import top.taurushu.pojo.Event;
import top.taurushu.utils.FromKafkaSource;

import java.sql.Timestamp;
import java.time.Duration;
import java.util.HashSet;
import java.util.function.Function;

public class ForAggregateFullWindowFunction {
    public static void main(String[] args) throws Exception {
        Function<SingleOutputStreamOperator<Event>, Void> function = (SingleOutputStreamOperator<Event> stream) -> {
            stream.assignTimestampsAndWatermarks(
                            WatermarkStrategy.<Event>forBoundedOutOfOrderness(Duration.ofMillis(120)).withTimestampAssigner(
                                    (SerializableTimestampAssigner<Event>) (element, recordTimestamp) -> element.getTime()
                            )
                    )
                    .keyBy(Event::getName)
                    .window(TumblingEventTimeWindows.of(Time.milliseconds(200)))
                    .aggregate(new CustomAggWinFunc(), new CustomProWinFunc())
                    .print();
            return null;
        };
        FromKafkaSource.executeFromKafkaSource(function);
    }

    static class CustomAggWinFunc implements AggregateFunction<Event, HashSet<String>, Long> {

        @Override
        public HashSet<String> createAccumulator() {
            return new HashSet<>();
        }

        @Override
        public HashSet<String> add(Event value, HashSet<String> accumulator) {
            accumulator.add(value.getName());
            return accumulator;
        }

        @Override
        public Long getResult(HashSet<String> accumulator) {
            return (long) accumulator.size();
        }

        @Override
        public HashSet<String> merge(HashSet<String> a, HashSet<String> b) {
            HashSet<String> set = new HashSet<>();
            set.addAll(a);
            set.addAll(b);
            return set;
        }
    }

    static class CustomProWinFunc extends ProcessWindowFunction<Long, String, String, TimeWindow> {
        @Override
        public void process(String s, ProcessWindowFunction<Long, String, String, TimeWindow>.Context context,
                            Iterable<Long> elements, Collector<String> out) {
            for (Long size : elements) {
                out.collect(new Timestamp(context.window().getStart())
                        + " ~ " + new Timestamp(context.window().getEnd())
                        + ": " + size);
                break;
            }
        }
    }
}
```

#### å…¶ä»–API

##### Trigger è§¦å‘å™¨

Trigger è§¦å‘å™¨ æ§åˆ¶çª—å£ä»€ä¹ˆæ—¶å€™è§¦å‘ï¼Œè®¾ç½®å®šæ—¶æœåŠ¡ï¼Œå†çª—å£ç»“æŸæ—¶é—´è¦åšçš„åŠ¨ä½œ

* onElementï¼šæ¯æ¥å—ä¸€ä¸ªæ•°æ®ï¼Œè§¦å‘ä¸€ä¸ªè¡Œä¸º
* onProcessingTimeï¼šå®šæ—¶å¤„ç†ï¼Œæ‰§è¡Œä¸€ä¸ªè¡Œä¸º
* onEventTimeï¼šäº‹ä»¶æ—¶é—´ï¼Œæ‰§è¡Œä¸€ä¸ªè¡Œä¸º

> TriggerResultæšä¸¾ç±»ï¼Œå®šä¹‰å‘é€å’Œæ¸…ç†åŠ¨ä½œ

##### Evictor ç§»é™¤å™¨

Evictor ç§»é™¤å™¨ å®šä¹‰çª—å£æ•°æ®çš„å–èˆ

* evictBeforeï¼šå®šä¹‰è¿ç®—ä¹‹å‰çš„æ•°æ®çš„å–èˆ
* evictAfterï¼šå®šä¹‰è¿ç®—ä¹‹åçš„æ•°æ®çš„å–èˆ

##### AllowedLateness å…è®¸å»¶è¿Ÿ

AllowedLateness å…è®¸å»¶è¿Ÿ åœ¨windowåè°ƒç”¨allowedLatenessï¼Œè¡¨ç¤ºå…è®¸å»¶è¿Ÿ

*  å®šä¹‰çš„(Time)Time.timeï¼Œè™½ç„¶å®šä¹‰æœŸé—´ï¼Œå‡ºç°çš„æ•°æ®ä¸ä¼šè¿›è¡Œè®¡ç®—äº†ï¼Œä½†æ˜¯åœ¨è®¡ç®—åè¿˜æ˜¯èƒ½å¤Ÿæ·»åŠ åˆ°çª—å£ä¹‹ä¸­ï¼Œå‚ä¸ä¸‹ä¸€æ¬¡çš„è®¡ç®—ï¼Œå…è®¸å»¶è¿Ÿåˆ°è¾¾ï¼Œè®©çœŸæ­£å…³é—­çª—å£çš„æ—¶é—´ï¼Œå†æ™šä¸€äº›

#### SideOutputLateData ä¾§è¾“å‡ºæµ

SideOutputLateData ä¾§è¾“å‡ºæµ å®šä¹‰è¿Ÿåˆ°çš„æ•°æ®å­˜å‚¨ä½ç½®ï¼Œè¿˜å¯ä»¥å°†ä»–æå–å‡ºæ¥ï¼ŒåŸºäºçª—å£å¤„ç†å®Œæˆåçš„DataStreamè°ƒç”¨GetSideOutput()ï¼Œä¼ å…¥å¯¹åº”æ ‡ç­¾è·å–è¿Ÿåˆ°æ•°æ®æ‰€åœ¨çš„ä¾§è¾“å‡ºæµã€‚

```java
OutputTag<Event> eventOutputTag = new OutputTag<Event>("eventLate");  // å®šä¹‰ä¾§è¾“å‡ºæµæ ‡ç­¾
DataStream<String> mainOutput = stream
    .assignTimestampsAndWatermarks(...)
    .keyBy(...)
    .window(...)
    .sideOutputLateData(eventOutputTag)  // ä¼ è¿›ä¾§è¾“å‡ºæµçš„æ ‡ç­¾ï¼Œè·å–æ•°æ®
    .aggregate(AggFunction,ProcessFunction);
operator.getSideOutput(eventOutputTag).print("outside");  // ä»è¿ç®—ç»“æœä¸­è¿”å›æµ‹æ•°æ®æµæ•°æ®ï¼Œå¹¶æ‰“å°
operator.print("main"); 
```

### æ€»ç»“

æ•°æ®å…è®¸è¿Ÿåˆ°çš„ä¸‰ç§æ‰‹æ®µ

1. WaterMarkï¼šä¼šä¸¥é‡å¯¼è‡´è®¡ç®—å»¶è¿Ÿï¼Œé€šå¸¸è®¾ç½® < 1000ms
2. allowLatenessï¼šå…è®¸æ•°æ®è¿Ÿåˆ°ï¼Œä½†æ˜¯çª—å£ä¸å…³é—­ï¼Œèµ„æºä¸èƒ½é‡Šæ”¾ï¼Œé€šå¸¸è®¾ç½® 1min
3. sideOutputStreamï¼šå°†ä¸¥é‡è¿Ÿåˆ°çš„æ•°æ®å†æ•´åˆåˆ°ä¸€èµ·

## å¤„ç†å‡½æ•°













































