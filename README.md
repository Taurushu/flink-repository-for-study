# Flink

â€‹	Apache Flink is a *framework*(æ¡†æ¶) and *distributed*(åˆ†å¸ƒå¼) processing engine for *stateful*(çŠ¶æ€) computations over *unbounded*(æ— ç•Œ) and *bounded data streams* [æ•°æ®æµä¸Šçš„æœ‰çŠ¶æ€è®¡ç®—]

## Flinkç®€ä»‹

#### åº”ç”¨åœºæ™¯

* ç”µå•†å’Œå¸‚åœºè¥é”€ï¼šå®æ—¶æŠ¥è¡¨ã€å¹¿å‘ŠæŠ•æ”¾ã€å®æ—¶æ¨è
* ç‰©è”ç½‘ï¼šå®æ—¶æ•°æ®é‡‡é›†ã€å®æ—¶æŠ¥è­¦
* ç‰©æµé…é€åŠæœåŠ¡ï¼šè®¢å•çŠ¶æ€è¿½è¸ªåŠæœåŠ¡
* é“¶è¡Œå’Œé‡‘èä¸šï¼šå®æ—¶ç»“ç®—ã€é£é™©æ£€æµ‹

#### ä¼˜åŠ¿

* æµæ‰¹ä¸€ä½“
* ä½å»¶è¿Ÿã€é«˜åå
* ç»“æœå‡†ç¡®æ€§å’Œè‰¯å¥½å®¹é”™æ€§

#### Flinkåˆ†å±‚çš„API

1. SQL [æœ€é«˜å±‚è¯­è¨€]
2. Table API [å£°æ˜å¼é¢†åŸŸä¸“ç”¨è¯­è¨€] (ç±»ä¼¼DSL)
3. `DataStream` / DataSet(å¼ƒç”¨è¾¹ç¼˜) API [æ ¸å¿ƒAPI]
4. æœ‰çŠ¶æ€æµå¤„ç† [åº•å±‚API] `ProcessFunction`

#### Sparkå’ŒFlink

| å¯¹æ¯”     | Spark               | Flink                            |
| -------- | ------------------- | -------------------------------- |
| æ¶æ„è®¾è®¡ | Lambda              | Kappa                            |
| åº•å±‚è®¾è®¡ | åŸºäºæ‰¹å¤„ç†è®¾è®¡      | åŸºäºæµå¤„ç†è®¾è®¡                   |
| æ•°æ®æ¨¡å‹ | RDD - å°æ•°æ®é‡é›†åˆ  | æ•°æ®æµ(DataStream)ã€äº‹ä»¶åºåˆ—     |
| è¿è¡Œæ¶æ„ | æ‰¹è®¡ç®—ï¼Œä»¥Stageåˆ’åˆ† | æ•°æ®ç›´æ¥å‘é€åˆ°ç›¸åº”çš„ç‚¹ï¼Œæ— éœ€ç­‰å¾… |

## Flinkå…¥é—¨æ¡ˆä¾‹

### Batch

```java
// 1.è·å–ç¯å¢ƒ
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

// 2.è·å–æ•°æ®æºã€è¯»å–æ•°æ®
DataSource<String> lineSource = env.readTextFile("input/words.txt");

// 3.é€šè¿‡lambdaè¡¨è¾¾å¼ï¼Œå°†lineSourceè½¬æ¢æˆTuple2æ ¼å¼ç„¶åæ”¶é›†èµ·æ¥ï¼Œæ‰å¹³æ˜ å°„å‡ºæ¥
FlatMapOperator<String, Tuple2<String, Long>> tuple2Return = lineSource.flatMap(
    (String line, Collector<Tuple2<String, Long>> out) -> {
        String[] str = line.split(" ");
        for (String s : str) {
            out.collect(Tuple2.of(s, 1L));
        }
    }
).returns(Types.TUPLE(Types.STRING, Types.LONG));

// 4.é€šè¿‡Tupleä¸­çš„ `0å·å…ƒç´ (key)` å¯¹ `1å·å…ƒç´ (value)` è¿›è¡ŒSumèšåˆç»Ÿè®¡
AggregateOperator<Tuple2<String, Long>> sum = tuple2Return.groupBy(0).sum(1);

// 5.æ‰“å°å‡ºç»“æœ
sum.print();
```

### Stream-Bounded

```java
// 1.è·å–ç¯å¢ƒ ã€readFileã€‘
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// 2.è·å–æ•°æ®æºã€è¯»å–æ•°æ®
DataStreamSource<String> lineSource = env.readTextFile("input/");

// 3.é€šè¿‡lambdaè¡¨è¾¾å¼ï¼Œå°†lineSourceè½¬æ¢æˆTuple2æ ¼å¼ç„¶åæ”¶é›†èµ·æ¥ï¼Œæ‰å¹³æ˜ å°„å‡ºæ¥
SingleOutputStreamOperator<Tuple2<String, Long>> tuple2Return = lineSource.flatMap(
    (String line, Collector<Tuple2<String, Long>> out) -> {
        String[] str = line.split(" ");
        for (String s : str) {
            out.collect(Tuple2.of(s, 1L));
        }
    }
).returns(Types.TUPLE(Types.STRING, Types.LONG));

// 4.é€šè¿‡Tupleä¸­çš„ `0å·å…ƒç´ (key)` å¯¹ `1å·å…ƒç´ (value)` è¿›è¡ŒSumèšåˆç»Ÿè®¡
SingleOutputStreamOperator<Tuple2<String, Long>> sum = tuple2Return.keyBy(data -> data.f0).sum(1);
// 5.æ‰“å°å‡ºç»“æœ
sum.print();

env.execute();
```

### Stream-unBounded

```java
// 1.è·å–ç¯å¢ƒ ã€socketã€‘
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// 2.è·å–æ•°æ®æºã€è¯»å–æ•°æ®
DataStreamSource<String> lineSource = env.socketTextStream("192.168.32.151", 11778);

// 3.é€šè¿‡lambdaè¡¨è¾¾å¼ï¼Œå°†lineSourceè½¬æ¢æˆTuple2æ ¼å¼ç„¶åæ”¶é›†èµ·æ¥ï¼Œæ‰å¹³æ˜ å°„å‡ºæ¥
SingleOutputStreamOperator<Tuple2<String, Long>> tuple2Return = lineSource.flatMap(
    (String line, Collector<Tuple2<String, Long>> out) -> {
        String[] str = line.split(" ");
        for (String s : str) {
            out.collect(Tuple2.of(s, 1L));
        }
    }
).returns(Types.TUPLE(Types.STRING, Types.LONG));

// 4.é€šè¿‡Tupleä¸­çš„ `0å·å…ƒç´ (key)` å¯¹ `1å·å…ƒç´ (value)` è¿›è¡ŒSumèšåˆç»Ÿè®¡
SingleOutputStreamOperator<Tuple2<String, Long>> sum = tuple2Return.keyBy(data -> data.f0).sum(1);
// 5.æ‰“å°å‡ºç»“æœ
sum.print();

env.execute();
```

## Flinkç¯å¢ƒæ­å»º

### ä¸Šä¼ èµ„æºï¼Œå®‰è£…java11ç¯å¢ƒ

> ä¸Šä¼  `flink-1.15.3-bin-scala_2.12.tgz` + `jdk-11.0.15.1_linux-x64_bin.tar.gz` åˆ° `/export/server` ç›®å½•ä¸‹
>
> å®‰è£… `Java11` å¹¶è§£å‹ `Flink1.15.3` 
>
> æ–°å»º flink ç”¨æˆ·ï¼Œä¿®æ”¹ flink ç”¨æˆ·çš„ `JAVA_HOME` ä¸ºJava11

### Standalone

ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­

```yaml
# ä¿®æ”¹jobmanagerçš„ä¸»èŠ‚ç‚¹åœ¨node1ä¸Š
jobmanager.rpc.address: node1
# ç»‘å®šæ‰€æœ‰ä¸»æœºéƒ½å¯ä»¥è®¿é—®æœ¬æœº
jobmanager.bind-host: 0.0.0.0

# é»˜è®¤webuiç»‘å®šåœ¨node1:8081ä¸Š ã€å¯æ³¨é‡Šã€‘
rest.port: 8081
rest.bind-address: node1
```

```shell
# å¯åŠ¨é›†ç¾¤
$FLINK_HOME/bin/start-cluster.sh

# åœæ­¢é›†ç¾¤
$FLINK_HOME/bin/stop-cluster.sh
```

### éƒ¨ç½²æ¨¡å¼

#### Session

ä¼šè¯æ¨¡å¼ï¼Œç‰¹ç‚¹æ˜¯èµ„æºå›ºå®šï¼Œä¸éœ€è¦é‡å¤å¯åŠ¨é›†ç¾¤å’Œèµ„æºï¼Œé€‚ç”¨äº`ä»»åŠ¡æ‰§è¡Œæ—¶é—´çŸ­ï¼Œä½œä¸šæ•°é‡å¤š`çš„åœºæ™¯ [standalone]

#### Per-Job

å•ä»»åŠ¡æ¨¡å¼ï¼Œç‰¹ç‚¹æ˜¯æ¯`ä¸€ä¸ªé›†ç¾¤åªé’ˆå¯¹ä¸€ä¸ªä»»åŠ¡`è¿›è¡Œè¿è¡Œï¼Œé€‚ç”¨äºä»»åŠ¡æ‰§è¡Œæ—¶é—´é•¿ï¼Œä½œä¸šæ•°é‡å°‘çš„åœºæ™¯ [yarnã€k8s]

#### Application

åº”ç”¨æ¨¡å¼ï¼Œç‰¹ç‚¹æ˜¯æ¯`ä¸€ä¸ªé›†ç¾¤åªé’ˆå¯¹ä¸€ä¸ªåº”ç”¨ç¨‹åº`è¿›è¡Œè¿è¡Œï¼Œé€‚ç”¨äºä»»åŠ¡æ‰§è¡Œæ—¶é—´é•¿ï¼Œä½œä¸šæ•°é‡å°‘çš„åœºæ™¯ [yarnã€k8s]

> other: `standaloneåœ¨applicationéƒ¨ç½²ä¸­`ï¼Œå¯ä»¥ä½¿ç”¨åŒæ ·åœ¨binç›®å½•ä¸‹çš„standalone-job.shæ¥åˆ›å»ºä¸€ä¸ªJobManager
>
> 1. è¿›å…¥åˆ°Flinkçš„å®‰è£…è·¯å¾„ä¸‹ï¼Œ`å°†åº”ç”¨ç¨‹åºçš„jaråŒ…æ”¾åˆ°lib/ç›®å½•ä¸‹`
>
> 	```shell
> 	cp ./FlinkTutorial-1.0-SNAPSHOT.jar lib/
> 	```
>
> 2. å¯åŠ¨JobManager
>
> 	```shell
> 	./bin/standalone-job.sh start --job-classname jarPath
> 	# ç›´æ¥æŒ‡å®šä½œä¸šå…¥å£ï¼Œè„šæœ¬ä¼šç›´æ¥æ‰«ælibç›®å½•ï¼Œæ‰€æœ‰çš„jaråŒ…
> 	```
>
> 3. å¯åŠ¨TaskManager
>
> 	```shell
> 	./bin/taskmanager.sh start
> 	```
>
> 4. åœæ­¢é›†ç¾¤
>
> 	```shell
> 	./bin/standalone-job.sh stop
> 	./bin/taskmanager.sh stop
> 	```

### Yarn

#### Session mode

åœ¨Yarné›†ç¾¤ä¸­å¼€å¯ä¸€ä¸ªé›†ç¾¤ï¼Œå¼€å¯Yarnä¼šè¯

```shell
bin/yarn-session.sh -nm appName
```

| å‚æ•°                     | å¤‡æ³¨                        |
| ------------------------ | --------------------------- |
| -d                       | åˆ†ç¦»æ¨¡å¼ï¼Œåå°è¿è¡Œ          |
| -jm(--jobManagerMemory)  | é…ç½®JobManagerå†…å­˜ï¼Œé»˜è®¤MB  |
| -nm(--name)              | é…ç½®YARN UIç•Œé¢çš„ä»»åŠ¡å     |
| -qu(--queue)             | æŒ‡å®šYARN é˜Ÿåˆ—å             |
| -tm(--taskManagerMemory) | é…ç½®TaskManagerå†…å­˜ï¼Œé»˜è®¤MB |

#### Per-Job mode

åœ¨Yarné›†ç¾¤ä¸­å¼€å¯ä¸€ä¸ªé›†ç¾¤ï¼Œç”¨äºæäº¤ä¸€ä¸ªå•ç‹¬çš„ä½œä¸šï¼Œå¯åŠ¨Flinké›†ç¾¤

```shell
# ç°åœ¨çš„ç‰ˆæœ¬
bin/flink run -d -t yarn-per-job -c top.taurushu.wc.WordCount FlinkWordCount.jar

# ä»¥å‰çš„ç‰ˆæœ¬ 
bin/flink run -m yarn-cluster -c top.taurushu.wc.WordCount FlinkWordCount.jar
```

| å‚æ•°                   | å¤‡æ³¨                                 |
| ---------------------- | ------------------------------------ |
| -d                     | åˆ†ç¦»æ¨¡å¼ï¼Œåå°è¿è¡Œ                   |
| -t                     | è¿è¡Œæ¨¡å¼ "yarn-per-job" (deprecated) |
| -c,--class <classname> | é…ç½®å¯åŠ¨ç±»å                         |

#### Application mode

åœ¨Yarné›†ç¾¤ä¸­å¼€å¯ä¸€ä¸ªé›†ç¾¤ï¼Œç”¨äºæäº¤ä¸€ä¸ªåº”ç”¨ç¨‹åºï¼Œå¯åŠ¨Flinké›†ç¾¤

```shell
# å¼€å¯é›†ç¾¤ï¼Œæäº¤ä½œä¸š
bin/flink run-application -t yarn-application -c top.taurushu.wc.WordCount FlinkWordCount.jar
# æŸ¥çœ‹é›†ç¾¤
bin/flink list -t yarn-application -Dyarn.application.id=application_xxxxxx_yy
# å–æ¶ˆä½œä¸š
bin/flink canel -t yarn-application -Dyarn.application.id=application_xxxxxx_yy <jobId>
```

æŠ¥é”™é—®é¢˜ï¼š

![image-20230402233336305](png/iak8pp.png)

> Exception in thread "Thread-5" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
> 
> è§£å†³æ–¹æ¡ˆï¼šåœ¨conf/flink-config.propertiesä¸­é…ç½®classloader.check-leaked-classloader: false

## ç³»ç»Ÿæ¶æ„

### Flinkè¿è¡Œæ—¶æ¶æ„

![jobæäº¤æŠ½è±¡æµç¨‹](png/tzicng-1.png)

### ä½œä¸šæäº¤æµç¨‹

#### jobæäº¤æŠ½è±¡æµç¨‹

![jobæäº¤æŠ½è±¡æµç¨‹](png/tuqhe3-1.png)

#### Standaloneæäº¤æµç¨‹

<img src="png/tzp5qu-1.png" alt="Standaloneæäº¤æµç¨‹" style="zoom: 33%;" />

#### Yarn-sessionæäº¤æµç¨‹

<img src="png/txns4p-1.png" alt="Yarn-sessionæäº¤æµç¨‹" style="zoom: 50%;" />

#### Yarn-Per-Jobæäº¤æµç¨‹

<img src="png/txxfe0-1.png" alt="Yarn-Per-Jobæäº¤æµç¨‹" style="zoom:50%;" />

### ç¨‹åºå’Œæ•°æ®æµ(DataFlow)

ä¸€ä¸ªFlinkç¨‹åºå¯ä»¥çœ‹ä½œä¸‰éƒ¨åˆ†ï¼šsourceã€transformationã€sink

Flinkç¨‹åºåœ¨è¿è¡Œæ—¶ï¼Œä¼šè¢«è½¬æ¢ä¸ºé€»è¾‘æ•°æ®æµ(DataFlows)åŒ…å«äº†ä¸‰éƒ¨åˆ†ï¼Œä»¥ä¸€ä¸ªæˆ–å¤šä¸ªsourceå¼€å§‹ï¼Œä»¥ä¸€ä¸ªæˆ–å¤šä¸ªsinkç»“æŸã€‚

### å¹¶è¡Œåº¦(Parallelism)

å¹¶è¡Œåº¦åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š`æ•°æ®å¹¶è¡Œåº¦`å’Œ`ä»»åŠ¡å¹¶è¡Œåº¦`

è®¾ç½®å¹¶è¡Œåº¦çš„æ–¹å¼:

```java
// 1. flink-config.properties
// parallelism.default: 1

// 2. å‘½ä»¤è¡Œå‚æ•°è®¾ç½®
// Parallelism=2

// 3. ä»£ç ç¯å¢ƒè®¾ç½®
env.setParallelism(2)
    
// 4. ä»£ç è¿è¡Œæ—¶æ·»åŠ ç®—å­å¹¶è¡Œåº¦ï¼Œä¼˜å…ˆçº§æœ€é«˜
Operator().setParallelism(2)
```

#### æ•°æ®ä¼ è¾“å½¢å¼

1. ä¸€ä¸ªç¨‹åºä¸­ï¼Œä¼šåŒ…å«å¤šä¸ªç®—å­ï¼Œä¸åŒçš„ç®—å­ä¼šå­˜åœ¨ä¸åŒçš„å¹¶è¡Œåº¦
2. ç®—å­é—´ï¼Œæ•°æ®å¯ä»¥æ˜¯one-to-one(forwarding)ï¼Œä¹Ÿå¯ä»¥æ˜¯redistributingï¼Œå–å†³äºç®—å­ç§ç±»
	* one-to-one(forwarding)ï¼šæ„å‘³ç€ï¼Œä¸¤ä¸ªç®—å­ä¹‹é—´ç»´æŠ¤çš„å…ƒç´ ä¸ªæ•°ã€é¡ºåºéƒ½ç›¸åŒï¼Œæ˜¯ä¸€å¯¹ä¸€å…³ç³»
	* redistributingï¼šstreamçš„åˆ†åŒºå‘ç”Ÿæ”¹å˜ï¼Œæ•°æ®ä¼šä¾æ®é€»è¾‘ï¼Œå‘é€åˆ°ä¸åŒçš„ç›®æ ‡ä»»åŠ¡ä¸­å»

#### ç®—å­é“¾

Flinkçš„ä»»åŠ¡é“¾ä¼˜åŒ–æŠ€æœ¯ï¼Œåœ¨å¹¶è¡Œåº¦ç›¸åŒã€one-to-oneæ—¶å¯¹ä»»åŠ¡é“¾è¿›è¡Œåˆå¹¶æ“ä½œï¼Œå‡å°‘æœ¬åœ°é€šä¿¡çš„å¼€é”€ï¼Œä»¥æœ¬åœ°è½¬å‘(local forward)çš„æ–¹å¼è¿›è¡Œè¿æ¥

#### æ‰§è¡Œå›¾

Flinkä¸­çš„æ‰§è¡Œå›¾åˆ†ä¸ºå››å±‚ï¼šStreamGraph -> JobGraph -> ExecutionGraph -> ç‰©ç†æ‰§è¡Œå›¾

* StreamGraph: ä¾æ®ä»£ç é€»è¾‘APIç”Ÿæˆçš„å›¾ï¼Œç”¨æ¥è¡¨ç¤ºç¨‹åºçš„æ‹“æ‰‘ç»“æ„
* JobGraph: ä¼˜åŒ–åçš„StreamGraphï¼Œå°†è¦æäº¤ç»™JobManagerçš„æ•°æ®ç»“æ„ã€‚ä¸»è¦å°†ç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹chainåœ¨ä¸€èµ·è¿›è¡Œä¼˜åŒ–
* ExecutionGraph: JobManagerç”Ÿæˆçš„Graphï¼ŒExecutionGraphæ˜¯JobGraphçš„å¹¶è¡ŒåŒ–ç‰ˆæœ¬ï¼Œæ˜¯è°ƒåº¦å±‚æœ€æ ¸å¿ƒçš„æ•°æ®ç»“æ„
* ç‰©ç†æ‰§è¡Œå›¾: JobManageræ ¹æ®ExecutionGraphè°ƒåº¦åï¼Œåœ¨TaskManagersä¸Šéƒ¨ç½²Taskåçš„å›¾ï¼Œä¸æ˜¯å…·ä½“çš„æ•°æ®ç»“æ„

![ExecutionGraphæ‰§è¡Œå›¾](png/ExecutionGraphæ‰§è¡Œå›¾.png)

#### Task \ TaskSlots

åœ¨Flinkä¸­ï¼Œä¸€ä¸ªTaskManager -> ä¸€ä¸ªJVMè¿›ç¨‹ -> nä¸ªTaskSlot -> mä¸ªTaskä»»åŠ¡ 

##### ä»»åŠ¡å…±äº«Slot

ç”±äºTaskSlotçš„èµ„æºç›¸ç­‰ï¼ŒTaskçš„è´Ÿè½½ä¸å‡ï¼Œè‹¥ä¸€ä¸ªTaskå ç”¨ä¸€ä¸ªTaskSlotï¼Œé‚£ä¹ˆé›†ç¾¤èµ„æºåˆ©ç”¨ç‡åˆ™ä¸é«˜ï¼Œæ‰€ä»¥Flinké»˜è®¤å¯ä»¥ä½¿ä¸åŒçš„Taskè¿è¡Œåœ¨åŒä¸€ä¸ªTaskSlotä¸­ï¼Œå«åšä»»åŠ¡å…±äº«Slot

è¿™æ ·å¯ä»¥ä½¿ä¸€ä¸ªslotè¿è¡ŒJobçš„æ•´ä¸ªç®¡é“ï¼Œè¿™æ ·`èµ„æºå¯†é›†å‹`å’Œ`éå¯†é›†å‹`çš„ä»»åŠ¡åŒæ—¶æ”¾åœ¨ä¸€ä¸ªslotä¸­å¯ä»¥è‡ªè¡Œè¯„åˆ†èµ„æºå ç”¨æ¯”ä¾‹ï¼Œå¹³è¡¡TaskManagerçš„è´Ÿè½½

1. Task Slot
	* é™æ€æ¦‚å¿µï¼ŒTaskManagerå…·æœ‰çš„å¹¶å‘æ‰§è¡Œèƒ½åŠ›
	* é€šè¿‡å‚æ•°taskmanager.numberOfTaskSlotsè¿›è¡Œé…ç½®
2. å¹¶è¡Œåº¦ (parallelism)
	* åŠ¨æ€æ¦‚å¿µï¼Œä¹Ÿå°±æ˜¯TaskManagerè¿è¡Œæ—¶å®é™…çš„å¹¶å‘èƒ½åŠ›
	* é€šè¿‡å‚æ•°parallelism.defaultè¿›è¡Œé…ç½®

è®¾ç½®å¹¶è¡Œåº¦çš„ä¸‰ç§å¸¸è§æ–¹å¼ï¼š

1. flink-conf.yaml: - yaml

	```yaml
	parallelism.default: 2
	```

2. Flink å®¢æˆ·ç«¯ - shell

	```shell
	bin/flink run -p 2
	```

3. ä»£ç æ‰§è¡Œç¯å¢ƒ - java

	```java
	env.setParallelism(2)
	```

4. ç®—å­æ‰§è¡Œå•ç‹¬ä¿®æ”¹ - java

	```java
	Operator().setParallelism(2)
	```

#### Other - å…³é—­ä¼˜åŒ–

å…³é—­é»˜è®¤ä¼˜åŒ– - chain

```java
// å…³é—­å‰åæ‰§è¡Œé“¾ä¼˜åŒ–
Operator().disableChaining();
env.disableChaining();

// å¼€å¯å½“å‰ç®—å­å‰åæ‰§è¡Œé“¾ä¼˜åŒ–
Operator().startNewChain();
env.startNewChain();
```

å…³é—­é»˜è®¤ä¼˜åŒ– - sharing slot

```java
// è®¾ç½®æŸç®—å­çš„slotå…±äº«ç»„
Operator().slotSharingGroup(String str)
// 1. åŒç»„çš„ç®—å­ï¼Œæ‰å¯ä»¥å…±äº«slot
// 2. è°ƒç”¨äº†slotSharingGroup()åï¼Œè¯¥ç®—å­åé¢çš„æ‰€æœ‰ç®—å­éƒ½æ˜¯è¯¥ç»„
```

## DataStream Api

Flinkçš„åŸºæœ¬æ­¥éª¤

1. è·å–æ‰§è¡Œç¯å¢ƒ(execution environment)
2. è¯»å–æ•°æ®æº(source)
3. å®šä¹‰åŸºäºæ•°æ®çš„è½¬æ¢æ“ä½œ(transformations)
4. å®šä¹‰è®¡ç®—ç»“æœçš„è¾“å‡ºä½ç½®(sink)
5. è§¦å‘ç¨‹åºæ‰§è¡Œ(execute)

![image-20230403213813630](png/zczntl-1.png)

### åˆ›å»ºæ‰§è¡Œç¯å¢ƒ

1. åˆ›å»ºæ‰¹å¼æ‰§è¡Œç¯å¢ƒ

	```java
	ExecutionEnvironment env =
	ExecutionEnvironment.getExecutionEnvironment()
	```

2. åˆ›å»ºæµå¼æ‰§è¡Œç¯å¢ƒ

	```java
	// è‡ªåŠ¨æ ¹æ®ä¸Šä¸‹æ–‡è·å–æ‰§è¡Œç¯å¢ƒï¼Œæœ¬æœºæ‰§è¡Œåˆ™æ˜¯æœ¬åœ°ï¼Œè¿è¡Œflinkç¨‹åºåˆ™æ˜¯è¿œç¨‹é›†ç¾¤æœåŠ¡
	StreamExecutionEnvironment env =
	StreamExecutionEnvironment.getExecutionEnvironment()
	    
	// åˆ›å»ºæœ¬åœ°æ‰§è¡Œç¯å¢ƒ
	StreamExecutionEnvironment localEnv =
	StreamExecutionEnvironment.createLocalEnvironment()
	    
	// åˆ›å»ºè¿œç¨‹é›†ç¾¤æ‰§è¡Œç¯å¢ƒ
	StreamExecutionEnvironment env =
	StreamExecutionEnvironment.createRemoteEnvironment(
	    hostname: "host",
	    port: 1234,
	    jar: "path/.jar"
	)
	```

3. ä»¥æµå¼Apiè¿è¡Œæ‰¹å¤„ç†
	DataStreamçš„ä¸‰ç§æ‰§è¡Œæ¨¡å¼

	* streaming: 

	* batch: 

	* automatic: 

	1. ä»¥javaä»£ç æŒ‡å®šï¼ˆä»£ç å†…ç½®å†™æ­»ï¼Œä¸æ¨èï¼‰

	```java
	env.setRuntimeMode(RuntimeExecutionMode.BATCH);
	```

	2. å‘½ä»¤è¡Œæäº¤ä»£ç æ—¶ï¼Œè¿›è¡Œé…ç½®

	```shell
	bin/flink run -Dexecution.runtime-mode=BATCH ...
	```

	> è™½ç„¶BATCHæ¨¡å¼çœ‹èµ·æ¥ä¼¼ä¹è¢«Streamingå…¨è¦†ç›–äº†
	>
	> åŒºåˆ«æ˜¯BATCHè¿è¡Œæ—¶ï¼Œæ•°æ®å…¨éƒ¨å¤„ç†å®Œï¼Œæ‰ä¼šä¸€æ¬¡æ€§è¾“å‡ºç»“æœ
	>
	> ä½†æ˜¯Streamè¿è¡Œä¸­ï¼Œæµå¼å¤„ç†æ¨¡å¼å°†ä¼šäº§ç”Ÿæ›´å¤šçš„ä¸­é—´ç»“æœè¾“å‡º
	>
	> æ‰€ä»¥åœ¨æœ¬æ¥è¾“å…¥æœ‰ç•Œã€åªå¸Œæœ›é€šè¿‡æ‰¹å¤„ç†å¾—åˆ°æœ€ç»ˆç»“æœçš„åœºæ™¯ä¸‹ï¼ŒStreamingæ¨¡å¼ä¸‹ä¼šä¸å¤Ÿé«˜æ•ˆ

### DataSource

#### readTextFile / socketTextStream

```java
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
// env.setParallelism(1);
DataStreamSource<String> textFile = env.readTextFile("input/UserEvent.log");
// DataStreamSource<String> socketTextStream = env.socketTextStream("node1", 17788);

// å°è£…å¤„ç†é€»è¾‘
SingleOutputStreamOperator<Event> map = textFile
    .map(s -> new Event(s.split(",")[0], s.split(",")[1], Long.valueOf(s.split(",")[2])));

map.print();
env.execute();
```

#### KafkaSource

<1.15.3å®˜ç½‘æ›´æ–°ç‰ˆæœ¬>

1. å¯¼å…¥flinkè¿æ¥kafkaçš„ä¾èµ–

	```xml
	<dependency>
	    <groupId>org.apache.flink</groupId>
	    <artifactId>flink-connector-kafka</artifactId>
	    <version>${flink.veresion}</version>
	</dependency>
	```

2. é…ç½®kafkaSourceï¼Œè¯»å–æ•°æ®

	```java
	/* è¯»å–kafkaå†…å®¹ */
	KafkaSource<String> source = KafkaSource.<String>builder()
	    .setBootstrapServers("node1:9092")
	    .setTopics("input-topic")
	    .setGroupId("my-group")
	    .setStartingOffsets(OffsetsInitializer.earliest())
	    .setValueOnlyDeserializer(new SimpleStringSchema())
	    .build();
	
	DataStreamSource<String> kafkaSource = 
	    env.fromSource(source, WatermarkStrategy.noWatermarks(), "Kafka Source");
	```

#### DIY Source



1. ParallelSourceFunction

	```java
	StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
	DataStreamSource<Event> source = env.addSource(new SourceFunction<Event>() {
	    private Boolean running = true;
	    @Override
	    public void run(SourceContext<Event> sourceContext) {
	        Random random = new Random();
	        String[] users = {"Mary", "Lily", "Bob", "Alix"};
	        String[] urls = {"./home", "./math", "./product?id=2232"};
	        while (running) {
	            sourceContext.collect(new Event(
	                users[random.nextInt(users.length)],
	                urls[random.nextInt(urls.length)],
	                new Date().getTime()
	            ));
	        }
	    }
	
	    @Override
	    public void cancel() {
	        running = false;
	    }
	});
	
	source.print();
	env.execute();
	```

2. ParallelSourceFunction

	```java
	StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
	env.setParallelism(4);
	// ä¿®æ”¹class - SourceFunction ä¸º ParallelSourceFunction
	DataStreamSource<Event> source = env.addSource(new ParallelSourceFunction<Event>() {
			...
	
	    @Override
	    public void run(SourceContext<Event> sourceContext) {
			...
	    }
	
	    @Override
	    public void cancel() {
			...
	    }
	}).setParallelism(2); // å¯ä»¥è®¾ç½®å¹¶è¡Œåº¦ï¼Œä½¿å…¶å¹¶è¡Œè¯»å–
	
	source.print();
	env.execute();
	```

### SingleOutputStreamOperator

#### BasicTransformation

##### Map

è¿›è¡Œ`æ˜ å°„`æ“ä½œï¼Œå¸¸ç”¨äºè½¬æ¢ç±»å‹ï¼Œæˆ–è€…è·å–åˆ°æ•°æ®ä¸­éœ€è¦çš„å†…å®¹ä½¿ç”¨

> public <R> SingleOutputStreamOperator<R> map(MapFunction<T, R> mapper)

  ```java
  SingleOutputStreamOperator<Event> map = textFile.map(
      s -> new Event(s.split(",")[0], s.split(",")[1], Long.valueOf(s.split(",")[2]))
  ).returns(Types.POJO(Event.class));
  ```

##### Filter

è¿›è¡Œè¿‡æ»¤æ“ä½œï¼Œè¿”å›trueçš„å…ƒç´ æ­£å¸¸è¾“å‡ºï¼Œfalseå…ƒç´ ä¼šè¢«æ‹¦æˆª

> public SingleOutputStreamOperator<T> filter(FilterFunction<T> filter)

```java
map = map.filter(value -> !"Bob".equals(value.getName()));
```

##### FlatMap

è¿›è¡Œ`æ‰å¹³æ˜ å°„`æ“ä½œï¼Œè¿”å›åˆ—è¡¨å†…å®¹ä¸­çš„æ‰€æœ‰å…ƒç´ 

> public <R> SingleOutputStreamOperator<R> flatMap(FlatMapFunction<T, R> flatMapper)

```java
SingleOutputStreamOperator<String> map = wordTxt.flatMap(
    (String value, Collector<String> out) -> Arrays.stream(value.split(" ")).forEach(out::collect)
).returns(Types.STRING);
```

#### AggregationTransformation

##### keyBy & reduce

æ ¹æ®ç»™å®šé€»è¾‘åˆ†åˆ«è¿›è¡Œåˆ†ç»„èšåˆï¼Œå±äºæ˜¯é€šç”¨é€»è¾‘ï¼Œä½†æ˜¯å®ç°å¤æ‚

> public <K> KeyedStream<T, K> keyBy(KeySelector<T, K> key)
>
> & public SingleOutputStreamOperator<T> reduce(ReduceFunction<T> reducer)

```java
SingleOutputStreamOperator<Tuple2<String, Long>> tuple2KeyValue = source.flatMap(
    (String value, Collector<Tuple2<String, Long>> out) ->
    Arrays.stream(value.split(" ")).forEach(
        word -> out.collect(new Tuple2<>(word, 1L))
    )
).returns(Types.TUPLE(Types.STRING, Types.LONG));

KeyedStream<Tuple2<String, Long>, String> tuple2StringKeyedStream = tuple2KeyValue.keyBy(value -> value.f0);

SingleOutputStreamOperator<Tuple2<String, Long>> keyed = tuple2StringKeyedStream.reduce(
    (Tuple2<String, Long> v1, Tuple2<String, Long> v2) -> Tuple2.of(v1.f0, v1.f1 + v2.f1)
).returns(Types.TUPLE(Types.STRING, Types.LONG));

keyed.keyBy(value -> "default").reduce(
    (Tuple2<String, Long> red1, Tuple2<String, Long> red2) -> red1.f1 > red2.f1 ? red1 : red2
).returns(Types.TUPLE(Types.STRING, Types.LONG)).print();
env.execute();
```

##### keyBy & max/maxBy

é€šè¿‡KeyByåˆ†ç»„ï¼Œæ±‚æœ€å¤§å€¼ï¼Œ

maxï¼šä¼šæ‰¾å‡ºä»¥maxä¸ºæœ€å¤§å€¼çš„maxï¼Œå…¶ä»–æ•°æ®ä¸ºåˆå§‹æ•°æ®

maxByï¼šä¼šæ‰¾å‡ºä»¥maxä¸ºæœ€å¤§å€¼çš„æ•°æ®

> public SingleOutputStreamOperator<T> max(int positionToMax)
>
> & public SingleOutputStreamOperator<T> maxBy(int positionToMaxBy)

```java
// hadoop hadoop hadoop
// shujie shujie shujie shujie
keyed.keyBy(value -> "default").max(1).returns(Types.TUPLE(Types.STRING, Types.LONG)).print();
// hadoop 4

keyed.keyBy(value -> "default").maxBy(1).returns(Types.TUPLE(Types.STRING, Types.LONG)).print();
// shujie 4
```

#### RichFunction

å¯Œå‡½æ•°ï¼šæä¾›æ¯”å¸¸è§„å‡½æ•°æ›´å¤šçš„ï¼Œæ›´ä¸°å¯ŒåŠŸèƒ½çš„ç±»ï¼Œå¯ä»¥è·å–åˆ°`è¿è¡Œç¯å¢ƒçš„ä¸Šä¸‹æ–‡`ï¼Œæ‹¥æœ‰ä¸€äº›`ç”Ÿå‘½å‘¨æœŸæ–¹æ³•`ç­‰ï¼Œå¯ä»¥å®ç°æ›´ä¸°å¯Œçš„åŠŸèƒ½ã€‚

#### ç‰©ç†åˆ†åŒº <Physical Partitioning>

åˆ†åŒºç­–ç•¥ï¼š

1. shuffleï¼šä»¥`éšæœºç­–ç•¥`å°†è‹¥å¹²æ•°æ®ï¼Œå‘é€åˆ°æ‰€æœ‰çš„åˆ†åŒºä¸­å»

	<img src="png/Flinkåˆ†åŒºShuffleç­–ç•¥.png" alt="Flinkåˆ†åŒºShuffleç­–ç•¥" style="zoom:150%;" />

2. rebalanceï¼šä»¥`è½®è¯¢ç­–ç•¥`å°†è‹¥å¹²æ•°æ®ï¼Œå‘é€åˆ°æ‰€æœ‰çš„åˆ†åŒºä¸­å»

	<img src="png/Flinkåˆ†åŒºBalanceç­–ç•¥.png" alt="Flinkåˆ†åŒºBalanceç­–ç•¥" style="zoom:150%;" />

3. rescaleï¼šä»¥`TaskManagerå†…è½®è¯¢ç­–ç•¥`å°†è‹¥å¹²æ•°æ®ï¼Œå‘é€åˆ°å½“å‰çš„TaskManagerçš„åˆ†åŒºä¸­å»

	<img src="png/Flinkåˆ†åŒºScaleç­–ç•¥.png" alt="Flinkåˆ†åŒºScaleç­–ç•¥" style="zoom:150%;" />

ç‰¹æ®Šåˆ†åŒºï¼š

1. broadcastï¼šä»¥`å¹¿æ’­ç­–ç•¥`å°†è‹¥å¹²æ•°æ®ï¼Œå‘é€åˆ°æ‰€æœ‰çš„åˆ†åŒºä¸­å»

	<img src="png/Flinkåˆ†åŒºBroadcastç­–ç•¥.png" alt="Flinkåˆ†åŒºBroadcastç­–ç•¥" style="zoom:150%;" />

2. globalï¼šä»¥`å…¨å±€ç­–ç•¥`å°†è‹¥å¹²æ•°æ®ï¼Œå‘é€åˆ°ä¸€ä¸ªåˆ†åŒºä¸­å»

	<img src="png/Flinkåˆ†åŒºGlobalç­–ç•¥.png" alt="Flinkåˆ†åŒºGlobalç­–ç•¥" style="zoom:150%;" />

###  Sink

#### LocalFileSystemSink

```java
SingleOutputStreamOperator<String> returns = ... ;

StreamingFileSink<String> sink = StreamingFileSink.<String>forRowFormat(
    	new Path("src/main/resources/output"),		// é…ç½®æ–‡ä»¶è¾“å‡ºè·¯å¾„
    	new SimpleStringEncoder<>("UTF-8")			// é…ç½®ç¼–ç æ ¼å¼
)											// è¿”å›sink,ä½¿ç”¨é»˜è®¤çš„æ»šåŠ¨ç­–ç•¥
    .withRollingPolicy(
    	DefaultRollingPolicy.builder()
	    .withMaxPartSize(MemorySize.ofMebiBytes(10L)) //ç¼“å­˜10Mbyts
	    .withRolloverInterval(Duration.ofSeconds(60)) // 60sç”Ÿæˆä¸€ä¸ªæ–°æ–‡ä»¶
	    .withInactivityInterval(Duration.ofSeconds(5)) // å¦‚æœæ— äº‹ä»¶è§¦å‘
	    .build()
	).build();

returns.addSink(sink); // sink
```

#### KafkaSink

```xml
<!-- flink è¿æ¥ kafka çš„åŒ… -->
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-kafka</artifactId>
    <version>${flink.veresion}</version>
</dependency>
```

```java
SingleOutputStreamOperator<String> returns = ... ;

KafkaSink<String> sink = KafkaSink.<String>builder()
    .setBootstrapServers("node1:9092")	// kafkaé›†ç¾¤åœ°å€
    .setRecordSerializer(
    	KafkaRecordSerializationSchema
    		.builder()
    		.setTopic("output-topic")	// kafka topic
    		.setValueSerializationSchema(new SimpleStringSchema())
    		.build()
	)
    .setDeliverGuarantee(DeliveryGuarantee.AT_LEAST_ONCE) 	// è®¾ç½®è‡³å°‘æ¶ˆè´¹ä¸€æ¬¡
    .build();

returns.sinkTo(sink);
```

#### RedisSink

```xml
<!-- https://mvnrepository.com/artifact/org.apache.bahir/flink-connector-redis -->
<dependency>
    <groupId>org.apache.bahir</groupId>
    <artifactId>flink-connector-redis_2.11</artifactId>
    <version>1.1-SNAPSHOT</version>
</dependency>
```

```java
SingleOutputStreamOperator<Event> returns = ... ;

HashSet<InetSocketAddress> inetSocketAddressHashSet = new HashSet<>();
inetSocketAddressHashSet.add(new InetSocketAddress("node1", 7000));	// èŠ‚ç‚¹1ä¿¡æ¯
inetSocketAddressHashSet.add(new InetSocketAddress("node2", 7000)); // èŠ‚ç‚¹2ä¿¡æ¯
inetSocketAddressHashSet.add(new InetSocketAddress("node3", 7000)); // èŠ‚ç‚¹3ä¿¡æ¯

source.addSink(new RedisSink<>(
    // å¦‚æœæ˜¯åˆ†ç‰‡é›†ç¾¤çš„è¯ä½¿ç”¨é›†ç¾¤æ¨¡å¼çš„ï¼ˆFlinkJedisClusterConfigç±»ï¼‰
    // å¦‚æœæ˜¯å“¨å…µé›†ç¾¤çš„è¯ä½¿ç”¨å“¨å…µæ¨¡å¼çš„ï¼ˆFlinkJedisSentinelConfigç±»ï¼‰
    // å¦‚æœæ˜¯å•ç‚¹è®¿é—®çš„è¯ä½¿ç”¨ç‹¬ç«‹æ¨¡å¼çš„ï¼ˆFlinkJedisPoolConfigç±»ï¼‰
    new FlinkJedisClusterConfig.Builder().setNodes(inetSocketAddressHashSet).build(),
    new RedisMapper<Event>() {
        @Override
        public RedisCommandDescription getCommandDescription() {
            return new RedisCommandDescription(RedisCommand.HSET, "HASH_NAME", 3);
            // ç¼–å†™redisé…ç½®ä¿¡æ¯ï¼Œä½¿ç”¨ä»€ä¹ˆæ–¹å¼ï¼Œè¿‡æœŸäº‹ä»¶ï¼Œsetæ–¹å¼ç­‰
        }

        @Override
        public String getKeyFromData(Event event) {	 // ä»è¾“å…¥æ•°æ®ä¸­è·å–key
            return event.getName();
        }

        @Override
        public String getValueFromData(Event event) {	// ä»è¾“å…¥æ•°æ®ä¸­è·å–valueå€¼
            return event.toString();
        }
    }
));
```

#### ElasticsearchSink

```xml
<!-- flink è¿æ¥ elasticsearchçš„åŒ… -->
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-elasticsearch7</artifactId>
    <version>${flink.veresion}</version>
</dependency>
```

```java
// function
private static IndexRequest createIndexRequest(String element) {
    Map<String, Object> json = new HashMap<>();
    json.put("data", element);

    return Requests.indexRequest()
        .index("my-index")
        .id(element)
        .source(json);
}
```

```java
SingleOutputStreamOperator<String> returns = ... ;
returns.sinkTo(new Elasticsearch7SinkBuilder<String>()
               // å•æ¡æ•°æ®ä¸ºå•ä½æ’å…¥æ•°ï¼Œç¼“å­˜æ•°é‡
               .setBulkFlushMaxActions(1) 
               // è®¾ç½®esé›†ç¾¤åœ°å€
               .setHosts(
                   new HttpHost("192.168.32.151", 9200, "http"),
                   new HttpHost("192.168.32.152", 9200, "http"),
                   new HttpHost("192.168.32.153", 9200, "http"))
               // è®¾ç½®åˆ›å»ºç´¢å¼•ï¼Œä»¥åŠå…ƒç´ çš„idï¼Œsourceç­‰æ•°æ®æ¥æº
               .setEmitter(
                   (element, context, indexer) -> indexer.add(createIndexRequest(element)))
               .build()
              );

env.execute();
```

#### MysqlSink

å¯¼å…¥Flink jdbcè¿æ¥ï¼Œmysqlé©±åŠ¨jaråŒ…

```xml
 <!-- flink è¿æ¥ jdbcçš„åŒ… -->
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-jdbc</artifactId>
    <version>${flink.veresion}</version>
</dependency>
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>5.1.47</version>
</dependency>
```

```java
DataStreamSource<Event> source = env.addSource(new DiyParallelSourceFunc()).setParallelism(2);


SinkFunction<Event> sink = JdbcSink.sink(
    "insert into events (name, uri, time) values (?, ?, ?)",                       // mandatory
    (PreparedStatement preparedStatement, Event event) -> {
        preparedStatement.setString(1, event.getName());
        preparedStatement.setString(2, event.getName());
        preparedStatement.setLong(3, event.getTime());
    },// mandatory
    JdbcExecutionOptions.builder()
    .withBatchIntervalMs(200)             // optional: default = 0, meaning no time-based execution is done
    .withBatchSize(1000)                  // optional: default = 5000 values
    .withMaxRetries(5)                    // optional: default = 3
    .build(),                  // optional
    new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
    .withUrl("jdbc:mysql://node1:3306/flinkSink?useSSL=false")
    .withUsername("root")
    .withPassword("shujie")
    .withDriverName("com.mysql.jdbc.Driver")
    .build()                  // mandatory
);

source.addSink(sink);
```

#### DiySink model

```java
package top.taurushu.streamSink;

import org.apache.flink.api.common.eventtime.Watermark;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;
import top.taurushu.streamSource.DiyParallelSourceFunc;
import top.taurushu.streamSource.Event;

public class WriteDiySink {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource<Event> source = env.addSource(new DiyParallelSourceFunc()).setParallelism(2);


        source.addSink(new MySinkFunction());

        env.execute();
    }
}

class MySinkFunction extends RichSinkFunction<Event> {

    @Override
    public void open(Configuration parameters) throws Exception {
        super.open(parameters);
    }

    @Override
    public void invoke(Event value, Context context) throws Exception {
        super.invoke(value, context);
    }

    @Override
    public void writeWatermark(Watermark watermark) throws Exception {
        super.writeWatermark(watermark);
    }

    @Override
    public void finish() throws Exception {
        super.finish();
    }

    @Override
    public void close() throws Exception {
        super.close();
    }
}
```

## æ—¶é—´/æ°´å°/çª—å£

### Time(æ—¶é—´)

1. å¤„ç†æ—¶é—´(Processing Time)
		å¤„ç†æ—¶é—´ï¼ŒæŒ‡æ‰§è¡Œå¤„ç†æ“ä½œçš„æœºå™¨çš„`ç³»ç»Ÿæ—¶é—´`ã€‚

2. äº‹ä»¶æ—¶é—´(Event Time)
		äº‹ä»¶æ—¶é—´ï¼ŒæŒ‡äº‹ä»¶å‘ç”Ÿçš„æ—¶é—´ï¼Œä¹Ÿå°±æ˜¯`æ•°æ®ç”Ÿæˆçš„æ—¶é—´`ã€‚æ˜¯æ•°æ®çš„å±æ€§ï¼Œä¹Ÿå°±æ˜¯è¿™æ¡æ•°æ®è®°å½•çš„"æ—¶é—´æˆ³ Timestamp"

	â€‹	åœ¨äº‹ä»¶æ—¶é—´è¯­ä¹‰ä¸‹ï¼Œæˆ‘ä»¬å¯¹äºæ—¶é—´çš„è¡¡é‡ï¼Œå°±ä¸çœ‹ä»»ä½•æœºå™¨çš„ç³»ç»Ÿæ—¶é—´äº†ï¼Œè€Œæ˜¯ä¾èµ–äºæ•°æ®æœ¬èº«ã€‚ç”±äºåˆ†å¸ƒå¼ç³»ç»Ÿä¸­ç½‘ç»œ`ä¼ è¾“å»¶è¿Ÿ`çš„ä¸ç¡®å®šæ€§ï¼Œå®é™…åº”ç”¨ä¸­æˆ‘ä»¬è¦é¢å¯¹çš„æ•°æ®æµå¾€å¾€æ˜¯`ä¹±åº`çš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå°±ä¸èƒ½ç®€å•åœ°æŠŠæ•°æ®è‡ªå¸¦çš„æ—¶é—´æˆ³å½“ä½œæ—¶é’Ÿäº†ï¼Œè€Œéœ€è¦ç”¨å¦å¤–çš„æ ‡å¿—æ¥è¡¨ç¤ºäº‹ä»¶æ—¶é—´è¿›å±•ï¼Œåœ¨Flinkä¸­æŠŠå®ƒå«ä½œäº‹ä»¶æ—¶é—´çš„ "æ°´ä½çº¿ `Watermark`"

> Flink1.12ç‰ˆæœ¬åï¼Œé»˜è®¤çš„æ—¶é—´è®¾ç½®çš„æ˜¯`äº‹ä»¶æ—¶é—´Event Time`

### WaterMark(æ°´ä½çº¿/æ°´å°)

æ°´ä½çº¿çš„å«ä¹‰ï¼šåœ¨æ°´ä½çº¿ä¹‹åï¼Œä¸ä¼šå†å‡ºç°æ°´ä½çº¿ä¹‹å‰çš„æ—¶é—´(EventTime)ï¼Œä»£è¡¨æ°´ä½çº¿æ—¶ï¼Œæ•°æ®å·²ç»åˆ°é½äº†

#### ä¹±åºç­–ç•¥

ä½¿ç”¨`Flinkå†…ç½®ç­–ç•¥ - forBoundedOutOfOrderness with TimestampAssigner`ï¼Œ

åœ¨ä»£ç ä¸­ç”Ÿæˆ`ä¹±åºæœ‰ç•Œçš„æ ¹æ®ç»™å®šæ—¶é—´çš„`æ°´ä½çº¿ï¼š

```java
DataStream<Event> source = ... ;

SingleOutputStreamOperator<Event> kfkSrcWithWM2 =
        source.assignTimestampsAndWatermarks(
                WatermarkStrategy
                        .<Event>forBoundedOutOfOrderness(Duration.ofMillis(120))
                        .withTimestampAssigner(
                                (SerializableTimestampAssigner<Event>)
                                        (element, recordTimestamp) -> element.getTime()
                        )
        );
```

#### æœ‰åºç­–ç•¥

ä½¿ç”¨`Flinkå†…ç½®ç­–ç•¥ - forMonotonousTimestamps with TimestampAssigner`ï¼Œ

åœ¨ä»£ç ä¸­ç”Ÿæˆ`å•è°ƒæ—¶é—´`çš„æ°´ä½çº¿ï¼š

```java
DataStream<Event> source = ... ;

SingleOutputStreamOperator<Event> kfkSrcWithWM2 =
        source.assignTimestampsAndWatermarks(
                WatermarkStrategy
                        .<Event>forMonotonousTimestamps()  // ä¿®æ”¹ä¸ºå•è°ƒçš„æ—¶é—´
                        .withTimestampAssigner(
                                (SerializableTimestampAssigner<Event>)
                                        (element, recordTimestamp) -> element.getTime()
                        )
        );
```

#### è‡ªå®šä¹‰æ°´ä½çº¿ç­–ç•¥

`åŸºäºäº‹ä»¶`å‘é€æ°´å°çš„

```java
(WatermarkStrategy<Event>) context -> new WatermarkGenerator<Event>() {
    @Override
    public void onEvent(Event event, long eventTimestamp, WatermarkOutput output) {
        output.emitWatermark(new Watermark(event.getTime()));
    }

    @Override
    public void onPeriodicEmit(WatermarkOutput output) {

    }
}
```

`åŸºäºå‘¨æœŸ`å‘é€`å…è®¸è¿Ÿåˆ°çš„`æ°´å°

```java
(WatermarkStrategy<Event>) context -> new WatermarkGenerator<Event>() {
    private long maxTimestamp;

    @Override
    public void onEvent(Event event, long eventTimestamp, WatermarkOutput output) {
        maxTimestamp = Math.max(maxTimestamp, event.getTime());
    }

    @Override
    public void onPeriodicEmit(WatermarkOutput output) {
        output.emitWatermark(new Watermark(maxTimestamp - 120 - 1));
    }
}
```

### Window(çª—å£)

#### æŒ‰ç…§é©±åŠ¨ç±»å‹åˆ’åˆ†

<img src="png/é©±åŠ¨ç±»å‹åˆ†ç±»çª—å£.png" alt="é©±åŠ¨ç±»å‹åˆ†ç±»çª—å£" style="zoom:150%;" />

##### 1. TimeWindow æ—¶é—´çª—å£

ä»¥äº‹æ—¶é—´ç‚¹æ¥å®šä¹‰çª—å£çš„å¼€å§‹ä¸ç»“æŸï¼Œåˆ°è¾¾ç»“æŸæ—¶é—´åï¼Œçª—å£ä¸åœ¨æ”¶é›†æ•°æ®

##### 2. CountWindow è®¡æ•°çª—å£

ä»¥çª—å£çš„æ•°æ®ä¸ªæ•°ä½œä¸ºå¼€å§‹å’Œç»“æŸï¼Œåªéœ€è¦é™åˆ¶çª—å£å¤§å°ï¼Œå°±å¯ä»¥æŠŠæ•°æ®åˆ†é…åˆ°å¯¹åº”çš„çª—å£ä¸­

#### æŒ‰ç…§çª—å£åˆ†é…æ•°æ®çš„è§„åˆ™åˆ†ç±»

##### 1. æ»šåŠ¨çª—å£(Tumbling Window)

å¯¹åº”ç±» ğŸ‘‰[`Tumbling[EventTime/ProcessingTime]Windows`](#TumblingWindows)

æ»šåŠ¨çª—å£ç‰¹ç‚¹ï¼šçª—å£`å¤§å°å›ºå®š`ï¼Œå°†æ•°æ®`å‡åŒ€åˆ‡ç‰‡`ï¼Œçª—å£é—´`æ²¡æœ‰é‡å `ï¼Œ`ä¸€ä¸ª`æ•°æ®åˆ†é…å±äº`ä¸€ä¸ª`çª—å£

<img src="png/sv2kl9-1.png" alt="image-20230407174524482" style="zoom:67%;" />

##### 2. æ»‘åŠ¨çª—å£(Sliding Window)

å¯¹åº”ç±» ğŸ‘‰[`Sliding[EventTime/ProcessingTime]Windows`](#SlidingWindows)

æ»‘åŠ¨çª—å£ç‰¹ç‚¹ï¼šçª—å£`å¤§å°å›ºå®š`ï¼Œçª—å£é—´`å­˜åœ¨æ»‘åŠ¨æ­¥é•¿`ï¼Œçª—å£é—´`æœ‰`é‡å éƒ¨åˆ†ï¼Œ`ä¸€ä¸ª`æ•°æ®å¯èƒ½å±äº`å¤šä¸ª`çª—å£

<img src="png/szuu9a-1.png" alt="image-20230407175327973" style="zoom: 67%;" />

##### 3. ä¼šè¯çª—å£(Session Window)

å¯¹åº”ç±»[`[Processing/DynamicProcessing/DynamicEvent/Event]TimeSessionWindow`](#SessionWindow)

ä¼šè¯çª—å£ï¼šçª—å£å¤§å°`ä¸å›ºå®š`ï¼ŒæŒ‰ç…§ä¼šè¯`é—´éš”å¤§å°`ï¼Œè¿›è¡Œåˆ†å‰²çª—å£ï¼Œçª—å£é—´æœ‰é—´éš”ä¸”é—´éš”`å¤§äº`è®¾å®šçš„é—´éš”å¤§å°

<img src="png//x70gpm-1.png" alt="image-20230407200640168" style="zoom:67%;" />

##### 4. å…¨å±€çª—å£(Global Window)

å¯¹åº”ç±»[`GlobalWindows`]

å…¨å±€çª—å£ï¼šæŠŠç›¸åŒkeyçš„`æ‰€æœ‰æ•°æ®`éƒ½åˆ†é…åˆ°`ä¸€ä¸ªçª—å£`ä¸­ï¼Œé»˜è®¤ä¸ä¼šè§¦å‘è®¡ç®—ï¼Œéœ€è¦`è‡ªå®šä¹‰è§¦å‘å™¨`ï¼Œè¿›è¡Œè®¡ç®—å¤„ç†

<img src="png/xczzkl-1.png" alt="image-20230407201721620" style="zoom:67%;" />

### çª—å£åˆ†ç±»

#### æŒ‰é”®åˆ†åŒº Keyed

åŸºäºkeyByåå¯¹`KeyedStream<T, K>`å»ºç«‹çª—å£

```java
stream.keyBy(element -> element.getKey()).window(WindowAssigner<?> assigner).xxx()
```

#### æŒ‰é”®åˆ†åŒº Non-Keyed

åŸºäºDataStreamå¯¹`å…¨éƒ¨`æ•°æ®å»ºç«‹çª—å£ [è¿™æ ·çš„è¯ï¼Œå…¨å±€å¹¶è¡Œåº¦åˆ™æ˜¯1ï¼Œå®é™…åº”ç”¨ä¸­ä¸æ¨èä½¿ç”¨]

```java
stream.windowAll(...).xxx()
```

### çª—å£API

æ€»çš„æ¥è¯´ï¼Œä¸€ä¸ªçª—å£å‡½æ•°åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼š1.çª—å£åˆ’åˆ†é€»è¾‘ [WindowAssigner] 2.çª—å£è®¡ç®—é€»è¾‘ [WindowFunction]

![Flinkæµä¹‹é—´çš„è½¬æ¢](png/Flinkæµä¹‹é—´çš„è½¬æ¢.png)

#### çª—å£åˆ’åˆ†é€»è¾‘ [WindowAssigner]

å®šä¹‰çª—å£ç±»å‹ï¼Œçª—å£å¦‚ä½•åˆ’åˆ†

1. <a name="TumblingWindows">æ»šåŠ¨çª—å£ Tumbling[EventTime/ProcessingTime]Windows</a>

	```java
	// éœ€è¦ä¼ ä¸€ä¸ªå‚æ•° - æ»šåŠ¨çª—å£çš„çª—å£å¤§å°
	stream.keyBy(key -> "default")
	    .window(TumblingEventTimeWindows.of(Time.milliseconds(1000)))
	    // å¤„ç†æ—¶é—´çª—å£/ä¼šè¯æ—¶é—´
		// .window(TumblingProcessingTimeWindows.of(Time.milliseconds(1000)))
	    .reduce((ReduceFunction<Long>) Long::sum)
	    .print();
	```

2. <a name="SlidingWindows">æ»‘åŠ¨çª—å£ Sliding[EventTime/ProcessingTime]Windows</a>

	```java
	// éœ€è¦ä¼ ä¸€ä¸ªå‚æ•° - åˆ’åŠ¨çª—å£çš„çª—å£å¤§å°ï¼Œä»¥åŠçª—å£ç§»åŠ¨çš„æ­¥é•¿ 
	stream.keyBy(key -> "default")
	    .window(SlidingEventTimeWindows.of(Time.milliseconds(1000),Time.milliseconds(500)))
		// å¤„ç†æ—¶é—´çª—å£/ä¼šè¯æ—¶é—´
		// .window(SlidingProcessingTimeWindows.of(Time.milliseconds(1000),Time.milliseconds(500)))
	    .reduce((ReduceFunction<Long>) Long::sum)
	    .print();
	```

3. <a name="SessionWindow">ä¼šè¯çª—å£ [Processing/DynamicProcessing/DynamicEvent/Event]TimeSessionWindow</a>

4. `è®¡æ•°çª—å£ countWindow`

	```java
	// éœ€è¦ä¼ ä¸€ä¸ªå‚æ•° - è®¡æ•°çª—å£ä¸­æ¯ä¸ªçª—å£çš„å…ƒç´ æ•°é‡
	stream.keyBy(key -> "default")
	    .countWindow(200)
	    .reduce((ReduceFunction<Long>) Long::sum)
	    .print();
	```

#### çª—å£è®¡ç®—é€»è¾‘ [WindowedStream / WindowFunction]

å…¸å‹çš„å¢é‡èšåˆå‡½æ•°æœ‰ï¼šReduceFunctionå½’çº¦å‡½æ•° / AggregateFunction

å¸¸è§çš„æœ‰ï¼šmin/minBy/max/maxBy/`aggregate`/apply(è€ç‰ˆæœ¬ï¼ŒåŠŸèƒ½è¢«processå–ä»£)/`process`/`reduce`

| å‡½æ•°                                                         |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| `reduce`(ReduceFunction(V, V))                               | å°†ä¸¤ä¸ªå…ƒç´ ï¼ŒæŒ‰ç…§`è‡ªå®šä¹‰é€»è¾‘`å¤„ç†ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„å…ƒç´            |
| `aggregate`(AggregateFunction<IN,ACC,OUT>)                   | æ ¹æ®ä¼ å…¥çš„æ³›å‹ï¼Œå®ç°çš„å››ä¸ªå‡½æ•°ï¼Œå®šä¹‰èšåˆå‡½æ•°                 |
| `aggregate`(AggregateFunction<IN,ACC,OUT>,<br/>ProcessWindowFunction<IN, OUT, KEY, W>) | ä¸¤ä¸ªå‡½æ•°ç›¸é…åˆä½¿ç”¨ï¼Œåœ¨aggregateå‡½æ•°ä¸­è®¡ç®—æ•°æ®ï¼Œåœ¨processå‡½æ•°å‘é€æ•°æ® |
| `process`(ProcessWindowFunction<IN, OUT, KEY, W>)            | æ ¹æ®ä¼ å…¥æ³›å‹å€¼åœ¨processå‡½æ•°ä¸­è¿›è¡Œè®¡ç®—ï¼Œå‘é€æ•°æ®              |

> * `reduce`:
> 	* ReduceFunction(V, V) -> V
> 		* Vï¼šè¿­ä»£è®¡ç®— value ç±»å‹ï¼Œè¿”å›valueç›¸åŒç±»å‹ï¼Œé€‚ç”¨äºç›¸åŒæ•°æ®é—´çš„è¿­ä»£è®¡ç®—
> * `aggregate`ï¼š
>   * AggregateFunction<IN, ACC, OUT> -> OUT
>   	* INï¼šè¾“å…¥æ•°æ®ç±»å‹
>   	* ACCï¼šç´¯åŠ å™¨ç±»å‹
>   	* OUTï¼šè¾“å‡ºæ•°æ®ç±»å‹
>   * æ–¹æ³•ï¼š
>   	* createAccumulateï¼šåˆå§‹åŒ–ç´¯åŠ å™¨
>   	* addï¼šaccumulatorï¼šè¿­ä»£è®¡ç®—
>   	* getResultï¼šè·å–è¿”å›å€¼
>   	* mergeï¼šçª—å£æ•°æ®åˆå¹¶ [`ä¼šè¯çª—å£`]
> * `process`ï¼š
> 	* ProcessWindowFunction<IN, OUT, KEY, W extends Window> -> OUT
> 		* INï¼šè¾“å…¥æ•°æ®ç±»å‹
> 		* OUTï¼šè¾“å‡ºæ•°æ®ç±»å‹
> 		* KEYï¼škeyByå­—æ®µåˆ†ç»„ç±»å‹
> 		* W extends Windowï¼šè®¾å®šçª—å£ç±»å‹
> * `aggregate`(AggregateFunction<T, ACC, V> aggFunction, 
> 	         ProcessWindowFunction<V, R, K, W> windowFunction)
> 	* AggregateFunction<T, ACC, V> -> V
> 		* Tï¼šè¾“å…¥çš„æ•°æ®ç±»å‹
> 		* ACCï¼šç´¯åŠ å™¨ç±»å‹
> 		* Vï¼šAggregateFunctionçš„æ•°æ®è¾“å‡ºç±»å‹å’ŒProcessWindowFunctionçš„æ•°æ®è¾“å…¥ç±»å‹
> 	* ProcessWindowFunction<V, R, K, W> -> R
> 		* Vï¼šAggregateFunctionçš„æ•°æ®è¾“å‡ºç±»å‹å’ŒProcessWindowFunctionçš„æ•°æ®è¾“å…¥ç±»å‹
> 		* Rï¼šè¿”å›æ•°æ®çš„ç±»å‹
> 		* Kï¼škeyByå­—æ®µç±»å‹
> 		* Wï¼šç»™å®šçª—å£ç±»å‹

```java
package top.taurushu.window;

import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.AggregateFunction;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;
import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.windows.TimeWindow;
import org.apache.flink.util.Collector;
import top.taurushu.pojo.Event;
import top.taurushu.utils.FromKafkaSource;

import java.sql.Timestamp;
import java.time.Duration;
import java.util.HashSet;
import java.util.function.Function;

public class ForAggregateFullWindowFunction {
    public static void main(String[] args) throws Exception {
        Function<SingleOutputStreamOperator<Event>, Void> function = (SingleOutputStreamOperator<Event> stream) -> {
            stream.assignTimestampsAndWatermarks(
                            WatermarkStrategy.<Event>forBoundedOutOfOrderness(Duration.ofMillis(120)).withTimestampAssigner(
                                    (SerializableTimestampAssigner<Event>) (element, recordTimestamp) -> element.getTime()
                            )
                    )
                    .keyBy(Event::getName)
                    .window(TumblingEventTimeWindows.of(Time.milliseconds(200)))
                    .aggregate(new CustomAggWinFunc(), new CustomProWinFunc())
                    .print();
            return null;
        };
        FromKafkaSource.executeFromKafkaSource(function);
    }

    static class CustomAggWinFunc implements AggregateFunction<Event, HashSet<String>, Long> {

        @Override
        public HashSet<String> createAccumulator() {
            return new HashSet<>();
        }

        @Override
        public HashSet<String> add(Event value, HashSet<String> accumulator) {
            accumulator.add(value.getName());
            return accumulator;
        }

        @Override
        public Long getResult(HashSet<String> accumulator) {
            return (long) accumulator.size();
        }

        @Override
        public HashSet<String> merge(HashSet<String> a, HashSet<String> b) {
            HashSet<String> set = new HashSet<>();
            set.addAll(a);
            set.addAll(b);
            return set;
        }
    }

    static class CustomProWinFunc extends ProcessWindowFunction<Long, String, String, TimeWindow> {
        @Override
        public void process(String s, ProcessWindowFunction<Long, String, String, TimeWindow>.Context context,
                            Iterable<Long> elements, Collector<String> out) {
            for (Long size : elements) {
                out.collect(new Timestamp(context.window().getStart())
                        + " ~ " + new Timestamp(context.window().getEnd())
                        + ": " + size);
                break;
            }
        }
    }
}
```

#### å…¶ä»–API

##### Trigger è§¦å‘å™¨

Trigger è§¦å‘å™¨ æ§åˆ¶çª—å£ä»€ä¹ˆæ—¶å€™è§¦å‘ï¼Œè®¾ç½®å®šæ—¶æœåŠ¡ï¼Œå†çª—å£ç»“æŸæ—¶é—´è¦åšçš„åŠ¨ä½œ

* onElementï¼šæ¯æ¥å—ä¸€ä¸ªæ•°æ®ï¼Œè§¦å‘ä¸€ä¸ªè¡Œä¸º
* onProcessingTimeï¼šå®šæ—¶å¤„ç†ï¼Œæ‰§è¡Œä¸€ä¸ªè¡Œä¸º
* onEventTimeï¼šäº‹ä»¶æ—¶é—´ï¼Œæ‰§è¡Œä¸€ä¸ªè¡Œä¸º

> TriggerResultæšä¸¾ç±»ï¼Œå®šä¹‰å‘é€å’Œæ¸…ç†åŠ¨ä½œ

##### Evictor ç§»é™¤å™¨

Evictor ç§»é™¤å™¨ å®šä¹‰çª—å£æ•°æ®çš„å–èˆ

* evictBeforeï¼šå®šä¹‰è¿ç®—ä¹‹å‰çš„æ•°æ®çš„å–èˆ
* evictAfterï¼šå®šä¹‰è¿ç®—ä¹‹åçš„æ•°æ®çš„å–èˆ

##### AllowedLateness å…è®¸å»¶è¿Ÿ

AllowedLateness å…è®¸å»¶è¿Ÿ åœ¨windowåè°ƒç”¨allowedLatenessï¼Œè¡¨ç¤ºå…è®¸å»¶è¿Ÿ

*  å®šä¹‰çš„(Time)Time.timeï¼Œè™½ç„¶å®šä¹‰æœŸé—´ï¼Œå‡ºç°çš„æ•°æ®ä¸ä¼šè¿›è¡Œè®¡ç®—äº†ï¼Œä½†æ˜¯åœ¨è®¡ç®—åè¿˜æ˜¯èƒ½å¤Ÿæ·»åŠ åˆ°çª—å£ä¹‹ä¸­ï¼Œå‚ä¸ä¸‹ä¸€æ¬¡çš„è®¡ç®—ï¼Œå…è®¸å»¶è¿Ÿåˆ°è¾¾ï¼Œè®©çœŸæ­£å…³é—­çª—å£çš„æ—¶é—´ï¼Œå†æ™šä¸€äº›

#### SideOutputLateData ä¾§è¾“å‡ºæµ

SideOutputLateData ä¾§è¾“å‡ºæµ å®šä¹‰è¿Ÿåˆ°çš„æ•°æ®å­˜å‚¨ä½ç½®ï¼Œè¿˜å¯ä»¥å°†ä»–æå–å‡ºæ¥ï¼ŒåŸºäºçª—å£å¤„ç†å®Œæˆåçš„DataStreamè°ƒç”¨GetSideOutput()ï¼Œä¼ å…¥å¯¹åº”æ ‡ç­¾è·å–è¿Ÿåˆ°æ•°æ®æ‰€åœ¨çš„ä¾§è¾“å‡ºæµã€‚

```java
OutputTag<Event> eventOutputTag = new OutputTag<Event>("eventLate");  // å®šä¹‰ä¾§è¾“å‡ºæµæ ‡ç­¾
DataStream<String> mainOutput = stream
    .assignTimestampsAndWatermarks(...)
    .keyBy(...)
    .window(...)
    .sideOutputLateData(eventOutputTag)  // ä¼ è¿›ä¾§è¾“å‡ºæµçš„æ ‡ç­¾ï¼Œè·å–æ•°æ®
    .aggregate(AggFunction,ProcessFunction);
operator.getSideOutput(eventOutputTag).print("outside");  // ä»è¿ç®—ç»“æœä¸­è¿”å›æµ‹æ•°æ®æµæ•°æ®ï¼Œå¹¶æ‰“å°
operator.print("main"); 
```

### æ€»ç»“

æ•°æ®å…è®¸è¿Ÿåˆ°çš„ä¸‰ç§æ‰‹æ®µ

1. WaterMarkï¼šä¼šä¸¥é‡å¯¼è‡´è®¡ç®—å»¶è¿Ÿï¼Œé€šå¸¸è®¾ç½® < 1000ms
2. allowLatenessï¼šå…è®¸æ•°æ®è¿Ÿåˆ°ï¼Œä½†æ˜¯çª—å£ä¸å…³é—­ï¼Œèµ„æºä¸èƒ½é‡Šæ”¾ï¼Œé€šå¸¸è®¾ç½® 1min
3. sideOutputStreamï¼šå°†ä¸¥é‡è¿Ÿåˆ°çš„æ•°æ®å†æ•´åˆåˆ°ä¸€èµ·

## å¤„ç†å‡½æ•°

åŸºæœ¬å¤„ç†å‡½æ•°ProcessFunction

åŸºæœ¬å¯ä»¥è·å–åˆ°æ‰€æœ‰çš„æ•°æ®ï¼š

* è·å–çª—å£å¯¹è±¡
* è·å–æ°´å°ä½ç½®
* è·å–äº‹ä»¶æ—¶é—´
* æ³¨å†Œå®šæ—¶æœåŠ¡
* å®ç°äº†å¯Œå‡½æ•°ç±»

### å¦‚ä½•è·å–

```java
DataStream stream = ... ;
stream.process(ProcessFunction<T, R> processFunction [, TypeInformation<R> outputType])
```

> public \<R\> SingleOutputStreamOperator\<R\> process(
>         ProcessFunction<T, R> processFunction, TypeInformation\<R\> outputType)

ProcessFunction

* processElement(IN, ProcessFunction<IN, OUT>.Context, Collector\<OUT\> )
  * Context: è·å–åˆ°ä¸Šä¸‹æ–‡å¯¹è±¡
  	* public abstract Long timestamp()ï¼šè·å–åˆ°å½“å‰äº‹ä»¶æ—¶é—´æˆ³
  	* public abstract TimerService timerService()ï¼šå®šæ—¶å™¨æœåŠ¡
  	  * long currentProcessingTime()ï¼šå½“å‰çš„å¤„ç†æ—¶é—´
  	  * long currentWatermark()ï¼šå½“å‰æ°´å°
  	  * void registerProcessingTimeTimer(long time)ï¼škeyedStreamæ‰èƒ½ä½¿ç”¨ï¼Œæ³¨å†Œå¤„ç†æ—¶é—´å®šæ—¶å™¨
  	  	* æ³¨å†Œå®šæ—¶å™¨ä¹‹åéœ€è¦å»å®ç°[onTimer](#onTimer)æ–¹æ³•
  	  * void registerEventTimeTimer(long time)ï¼škeyedStreamæ‰èƒ½ä½¿ç”¨ï¼Œæ³¨å†Œäº‹ä»¶æ—¶é—´å®šæ—¶å™¨
  	  	* æ³¨ å†Œå®šæ—¶å™¨ä¹‹åéœ€è¦å»å®ç°[onTimer](#onTimer)æ–¹æ³•
  	  * void deleteProcessingTimeTimer(long time)ï¼šåˆ é™¤å¤„ç†æ—¶é—´å®šæ—¶å™¨
  	  * void deleteEventTimeTimer(long time)ï¼šåˆ é™¤äº‹ä»¶æ—¶é—´å®šæ—¶å™¨
  	* public abstract \<X\> void output(OutputTag\<X\> outputTag, X value)ï¼šå‘é€åˆ°ä¾§è¾“å‡ºæµ
* getRuntimeContextï¼š
  * ä»»åŠ¡ï¼šè·å–Jobidã€è·å–Taskåã€æœ€å¤§å­ä»»åŠ¡æ•°
  * å­ä»»åŠ¡ï¼šè·å–å­ä»»åŠ¡ç´¢å¼•ã€è·å–å­ä»»åŠ¡çš„ä»»åŠ¡åã€è·å–åˆ†åŒºä¸­å­ä»»åŠ¡çš„æ•°é‡ã€è·å–æŒ‡æ ‡ç»„
  * å·¥å…·ï¼š[å»ºç«‹/è·å¾—]ç´¯åŠ å™¨ã€[åˆå§‹åŒ–è·å¾—/è·å¾—]å¹¿æ’­å˜é‡ã€è·å–åˆ†å¸ƒå¼ç¼“å­˜ã€è·å–åŒç²¾åº¦æµ®ç‚¹è®¡æ•°å™¨ã€è·å–é•¿æ•´æ•°è®¡æ•°å™¨ã€è·å–æ•´æ•°è®¡æ•°å™¨ã€
  * çŠ¶æ€ï¼šè·å–èšåˆçŠ¶æ€ã€è·å–å½’çº¦çŠ¶æ€ã€è·å–å°è¯•æ¬¡æ•°AttemptNumberã€è·å–çŠ¶æ€ã€è·å–åˆ—è¡¨çŠ¶æ€ã€è·å–MapçŠ¶æ€ã€æ˜¯å¦æœ‰å¹¿æ’­å˜é‡
  * å¯¹è±¡ï¼šè¿è¡Œé…ç½®å¯¹è±¡ã€è·å–å¤–éƒ¨èµ„æºä¿¡æ¯ã€è·å–ç›´æ–¹å›¾ã€è·å–ç”¨æˆ·ä»£ç ç±»åŠ è½½å™¨ã€å¦‚æ­Œä¸å­˜åœ¨åˆ™å®ä¾‹åŒ–é’©å­å‡½æ•°å¹¶æ³¨å†Œç”¨æˆ·ä»£ç ç±»åŠ è½½å™¨
* <a name="onTimer">onTimer</a>ï¼š
	* long timestamp: å½“å‰æ—¶é—´æˆ³
	* OnTimerContext ctcï¼šå®šæ—¶å™¨ä¸Šä¸‹æ–‡ = å®šæ—¶å™¨å†…å®¹ + ä¸Šä¸‹æ–‡
	* Output outï¼šæ•°æ®æ”¶é›†å™¨



### æ¡ˆä¾‹ï¼šæ¯ç§’è¾“å‡ºå½“å‰æœ€ç«çš„é¡µé¢

```java
/* top.taurushu.pojo.Event */
package top.taurushu.pojo;

import lombok.Data;

import java.sql.Timestamp;

@Data
public class Event {

    private String name;
    private String uri;
    private Long time;

    public Event() {
    }

    public Event(String value) {
        this(value.split(",")[0], value.split(",")[1], Long.valueOf(value.split(",")[2]));
    }

    public Event(String name, String uri, Long time) {
        this.name = name;
        this.uri = uri;
        this.time = time;
    }
    
    @Override
    public String toString() {
        return "Event{" +
                "name='" + name + '\'' +
                ", uri='" + uri + '\'' +
                ", time=" + new Timestamp(time) +
                '}';
    }

}
```

```java
/* top.taurushu.utils.FromKafkaSource; */
package top.taurushu.utils;

import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import top.taurushu.pojo.Event;

import java.util.function.Function;

public class FromKafkaSource {
    private FromKafkaSource() {

    }

    public static void executeFromKafkaSource(Function<SingleOutputStreamOperator<Event>, Void> function) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        KafkaSource<String> source = KafkaSource.<String>builder()
                .setBootstrapServers("node1:9092")
                .setTopics("flink-generate-topic")
                .setGroupId("my-group")
                .setStartingOffsets(OffsetsInitializer.earliest())
                .setValueOnlyDeserializer(new SimpleStringSchema())
                .build();
        SingleOutputStreamOperator<Event> kafkaSource = env.fromSource(source, WatermarkStrategy.noWatermarks(), "kafka Source")
                .map((MapFunction<String, Event>) Event::new).setParallelism(6);
        function.apply(kafkaSource);
        env.execute();
    }
}

```

```java
/* top.taurushu.pojo.UrlCount */
package top.taurushu.pojo;

import lombok.Data;

@Data
public class UrlCount {
    private String url;
    private Long count;
    private Long windowStart;
    private Long windowEnd;

    public UrlCount(){

    }

    public UrlCount(String url, Long count, Long windowStart, Long windowEnd) {
        this.url = url;
        this.count = count;
        this.windowStart = windowStart;
        this.windowEnd = windowEnd;
    }

    @Override
    public String toString() {
        return "UrlCount{" +
                "url='" + url + '\'' +
                ", count=" + count +
                ", windowStart=" + windowStart +
                ", windowEnd=" + windowEnd +
                '}';
    }
}
```

```java
/* top.taurushu.demo.ImplDemo1 */
package top.taurushu.demo;

import org.apache.flink.api.common.eventtime.SerializableTimestampAssigner;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.AggregateFunction;
import org.apache.flink.api.common.state.ListState;
import org.apache.flink.api.common.state.ListStateDescriptor;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.functions.KeyedProcessFunction;
import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;
import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.windows.TimeWindow;
import org.apache.flink.util.Collector;
import top.taurushu.pojo.Event;
import top.taurushu.pojo.UrlCount;
import top.taurushu.utils.FromKafkaSource;

import java.sql.Timestamp;
import java.time.Duration;
import java.util.ArrayList;
import java.util.function.Function;

public class ImplDemo1 {
    public static void main(String[] args) throws Exception {
        Function<SingleOutputStreamOperator<Event>, Void> function = (SingleOutputStreamOperator<Event> stream) -> {


            DataStream<UrlCount> aggregated = stream.assignTimestampsAndWatermarks(
                            WatermarkStrategy.<Event>forBoundedOutOfOrderness(Duration.ofMillis(50)).withTimestampAssigner(
                                    (SerializableTimestampAssigner<Event>) (element, recordTimestamp) -> element.getTime()
                            ))
                    .keyBy(Event::getUri)
                    .window(SlidingEventTimeWindows.of(Time.milliseconds(4000), Time.milliseconds(2000)))
                    .aggregate(new AggregateFunction<Event, UrlCount, UrlCount>() {
                                   @Override
                                   public UrlCount createAccumulator() {
                                       UrlCount urlCount = new UrlCount();
                                       urlCount.setUrl("");
                                       urlCount.setCount(0L);
                                       urlCount.setWindowStart(Long.MIN_VALUE);
                                       urlCount.setWindowEnd(Long.MIN_VALUE);
                                       return urlCount;
                                   }

                                   @Override
                                   public UrlCount add(Event value, UrlCount accumulator) {
                                       accumulator.setUrl(value.getUri());
                                       accumulator.setCount(accumulator.getCount() + 1L);
                                       return accumulator;
                                   }

                                   @Override
                                   public UrlCount getResult(UrlCount accumulator) {
                                       return accumulator;
                                   }

                                   @Override
                                   public UrlCount merge(UrlCount a, UrlCount b) {
                                       UrlCount urlCount = new UrlCount();
                                       urlCount.setUrl(a.getUrl());
                                       urlCount.setCount(a.getCount() + b.getCount());
                                       urlCount.setWindowStart(Math.min(a.getWindowStart(), b.getWindowStart()));
                                       urlCount.setWindowEnd(Math.max(a.getWindowEnd(), b.getWindowEnd()));
                                       return urlCount;
                                   }
                               },
                            new ProcessWindowFunction<UrlCount, UrlCount, String, TimeWindow>() {
                                @Override
                                public void process(String s, ProcessWindowFunction<UrlCount, UrlCount, String, TimeWindow>.Context context, Iterable<UrlCount> elements, Collector<UrlCount> out) throws Exception {
                                    for (UrlCount element : elements) {
                                        element.setWindowStart(context.window().getStart());
                                        element.setWindowEnd(context.window().getEnd());
                                        out.collect(element);
                                        break;
                                    }

                                }
                            }
                    );
//            aggregated.print("urlCount");

            aggregated.keyBy(UrlCount::getWindowEnd)
                    .process(new TopNProcessResult(3)).print("----------------------------");

            return null;
        };
        FromKafkaSource.executeFromKafkaSource(function);
    }

    public static class TopNProcessResult extends KeyedProcessFunction<Long, UrlCount, String> {

        private Integer n;
        private ListState<UrlCount> listState;


        public TopNProcessResult() {
        }

        public TopNProcessResult(Integer n) {
            this.n = n;
        }

        public Integer getN() {
            return n;
        }

        public void setN(Integer n) {
            this.n = n;
        }

        public static void getView(ArrayList<UrlCount> elements, int number, Collector<String> out,
                                   KeyedProcessFunction<Long, UrlCount, String>.Context ctx) {
            StringBuilder builder = new StringBuilder();
            builder.append("-----------------------------------------------\n");
            builder.append("çª—å£ç»“æŸæ—¶é—´: ")
                    .append(new Timestamp(ctx.getCurrentKey()))
                    .append('\n');
            for (int i = 0; i < Math.min(number, elements.size()); i++) {
                UrlCount element = elements.get(i);
                builder.append("No.").append(i + 1)
                        .append("\turl:'").append(element.getUrl())
                        .append("'\tè®¿é—®é‡:").append(element.getCount());
                builder.append('\n');
            }

            builder.append("-----------------------------------------------\n");
            out.collect(builder.toString());
        }

        @Override
        public void open(Configuration parameters) throws Exception {
            super.open(parameters);

            listState = getRuntimeContext()
                    .getListState(new ListStateDescriptor<UrlCount>("urlCountList", Types.POJO(UrlCount.class)));
        }

        @Override
        public void onTimer(long timestamp, KeyedProcessFunction<Long, UrlCount, String>.OnTimerContext ctx, Collector<String> out) throws Exception {
            ArrayList<UrlCount> urlCounts = new ArrayList<>();
            listState.get().forEach(urlCounts::add);
            urlCounts.sort((o1, o2) -> (int) (o2.getCount() - o1.getCount()));
            getView(urlCounts, 3, out, ctx);
        }

        @Override
        public void processElement(UrlCount value, KeyedProcessFunction<Long, UrlCount, String>.Context ctx, Collector<String> out) throws Exception {
            listState.add(value);
            ctx.timerService().registerEventTimeTimer(value.getWindowEnd() + 1L);
        }
    }
}
```

```java
/* top.taurushu.demo.ImplDemo2 */
package top.taurushu.demo;

import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.AggregateFunction;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.functions.windowing.ProcessAllWindowFunction;
import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.windows.TimeWindow;
import org.apache.flink.util.Collector;
import top.taurushu.pojo.Event;
import top.taurushu.utils.FromKafkaSource;

import java.sql.Timestamp;
import java.time.Duration;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.function.Function;

public class ImplDemo2 {
    public static void main(String[] args) throws Exception {
        Function<SingleOutputStreamOperator<Event>, Void> function = (SingleOutputStreamOperator<Event> stream) -> {
            stream.assignTimestampsAndWatermarks(
                            WatermarkStrategy
                                    .<Event>forBoundedOutOfOrderness(Duration.ofMillis(50))
                                    .withTimestampAssigner((Event element, long recordTimestamp) ->
                                            element.getTime()
                                    )
                    )
                    .map(Event::getUri)

                    .windowAll(SlidingEventTimeWindows.of(Time.milliseconds(1000), Time.milliseconds(500)))
                    .aggregate(new AggregateFunction<String, Map<String, Long>, List<Tuple2<String, Long>>>() {
                        @Override
                        public Map<String, Long> createAccumulator() {
                            return new HashMap<>();
                        }

                        @Override
                        public Map<String, Long> add(String value, Map<String, Long> accumulator) {
                            if (accumulator.containsKey(value)) {
                                accumulator.put(value, accumulator.get(value) + 1L);
                            } else {
                                accumulator.put(value, 1L);
                            }
                            return accumulator;
                        }

                        @Override
                        public List<Tuple2<String, Long>> getResult(Map<String, Long> accumulator) {
                            List<Map.Entry<String, Long>> list = new ArrayList<>(accumulator.entrySet());
                            list.sort((o1, o2) -> (int) (o2.getValue() - o1.getValue()));

                            ArrayList<Tuple2<String, Long>> returnList = new ArrayList<>();
                            for (int i = 0; i < 3; i++) {
                                if (list.size() > i) {
                                    returnList.add(Tuple2.of(list.get(i).getKey(), list.get(i).getValue()));
                                }
                            }
                            return returnList;
                        }

                        @Override
                        public Map<String, Long> merge(Map<String, Long> a, Map<String, Long> b) {
                            HashMap<String, Long> map = new HashMap<>();
                            map.putAll(a);
                            map.putAll(b);
                            return map;
                        }
                    }
                    , new ProcessAllWindowFunction<List<Tuple2<String, Long>>, String, TimeWindow>() {
                        @Override
                        public void process(ProcessAllWindowFunction<List<Tuple2<String, Long>>, String, TimeWindow>.Context context,
                                            Iterable<List<Tuple2<String, Long>>> elements,
                                            Collector<String> out) throws Exception {
                            StringBuilder builder = new StringBuilder();
                            builder.append("-----------------------------------------------\n");
                            for (List<Tuple2<String, Long>> element : elements) {
                                builder.append("çª—å£ç»“æŸæ—¶é—´: ")
                                        .append(new Timestamp(context.window().getEnd()))
                                        .append('\n');
                                for (int i = 0; i < element.size(); i++) {
                                    Tuple2<String, Long> tuple2 = element.get(i);
                                    builder.append("No.").append(i + 1)
                                            .append("\turl:'").append(tuple2.f0)
                                            .append("'\tè®¿é—®é‡:").append(tuple2.f1);
                                    builder.append('\n');
                                }
                                break;
                            }
                            builder.append("-----------------------------------------------\n");
                            out.collect(builder.toString());
                        }
                    }
                    ).print();


            return null;
        };


        FromKafkaSource.executeFromKafkaSource(function);
    }
}
```

## å¤šæµæ“ä½œ

### 1 -> N åˆ†æµæ“ä½œ

```java
public static void main(String[] args) throws Exception {
    Function<SingleOutputStreamOperator<Event>, Void> function = (SingleOutputStreamOperator<Event> stream) -> {

        stream = stream.assignTimestampsAndWatermarks(
            WatermarkStrategy.<Event>forBoundedOutOfOrderness(
                Duration.ofMillis(50)
            ).withTimestampAssigner((element, recordTimestamp) -> element.getTime())
        ).returns(Types.POJO(Event.class));
        OutputTag<Event> outMarry = new OutputTag<Event>("outMary", Types.POJO(Event.class));
        OutputTag<Event> outBob = new OutputTag<Event>("outBob", Types.POJO(Event.class));
        SingleOutputStreamOperator<Event> process = stream.process(new ProcessFunction<Event, Event>() {
            @Override
            public void processElement(Event value, ProcessFunction<Event, Event>.Context ctx,
                                       Collector<Event> out) throws Exception {
                if (Objects.equals(value.getName().substring(0, 4), "Mary")) {
                    ctx.output(outMarry, value);
                } else if (Objects.equals(value.getName().substring(0, 3), "Bob")) {
                    ctx.output(outBob, value);
                } else {
                    out.collect(value);
                }
            }
        }).returns(Types.POJO(Event.class));
        process.getSideOutput(outMarry).print("outMarry");
        process.getSideOutput(outBob).print("outBob");
        process.print("outElse");
        return null;
    };
    FromKafkaSource.executeFromKafkaSource(function);
}
```

### N -> 1 åˆæµæ“ä½œ

#### Union è”åˆæµ

```java
SingleOutputStreamOperator<Event> source1 = ... ;
SingleOutputStreamOperator<Event> source2 = ... ;

source1.<Event>union(source2).map((Event e) -> 1).keyBy(l -> 1).sum(0).print();
```

#### Connect è¿æ¥æµ

```java
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
DataStreamSource<Integer> ints = env.fromElements(1, 2, 3);
DataStreamSource<Long> longs = env.fromElements(1L, 2L, 3L);

ints.connect(longs).map(new CoMapFunction<Integer, Long, String>() {
    @Override
    public String map1(Integer value) throws Exception {
        return value.toString();
    }

    @Override
    public String map2(Long value) throws Exception {
        return value.toString();
    }
}).print();
env.execute();
```

#### boarodcastjoin å¹¿æ’­è¿æ¥æµ

æ¶‰åŠçŠ¶æ€çŸ¥è¯† ğŸ‘‰[boarodcastjoin](#boarodcastjoin)

#### å¯¹è´¦æ¡ˆä¾‹<çŠ¶æ€>

```java
package top.taurushu.union;

import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.state.ValueState;
import org.apache.flink.api.common.state.ValueStateDescriptor;
import org.apache.flink.api.scala.typeutils.Types;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.co.CoProcessFunction;
import org.apache.flink.util.Collector;
import top.taurushu.pojo.Pay;
import top.taurushu.pojo.ThirdPayPlatform;

import java.time.Duration;
import java.util.Objects;

public class ElseConnectStream {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();


        SingleOutputStreamOperator<Pay> paySource = env.fromElements(
            new Pay("order_01", "app", 1000L),
            new Pay("order_02", "app", 1200L),
            new Pay("order_03", "app", 1300L),
            new Pay("order_04", "app", 1400L),
            new Pay("order_05", "app", 1500L)
        ).assignTimestampsAndWatermarks(WatermarkStrategy.<Pay>forBoundedOutOfOrderness(Duration.ofMillis(100))
                                        .withTimestampAssigner((element, recordTimestamp) -> element.getTimestampLong()));


        SingleOutputStreamOperator<ThirdPayPlatform> thirdSource = env.fromElements(
            new ThirdPayPlatform("order_01", "ThirdPayPlatform", true, 1000L - 100L),
            new ThirdPayPlatform("order_02", "ThirdPayPlatform", true, 1200L - 100L),
            new ThirdPayPlatform("order_03", "ThirdPayPlatform", true, 1300L - 100L),
            new ThirdPayPlatform("order_04", "ThirdPayPlatform", true, 1400L - 100L),
            new ThirdPayPlatform("order_05", "ThirdPayPlatform", true, 1500L - 100L)
        ).assignTimestampsAndWatermarks(WatermarkStrategy.<ThirdPayPlatform>forBoundedOutOfOrderness(Duration.ofMillis(100))
                                        .withTimestampAssigner((element, recordTimestamp) -> element.getTime()));


        paySource.connect(thirdSource)
            .keyBy(Pay::getOrderId, ThirdPayPlatform::getOrderId)
            .process(new CoProcessFunction<Pay, ThirdPayPlatform, String>() {
                ValueState<Pay> payValueState;
                ValueState<ThirdPayPlatform> platformValueState;

                @Override
                public void open(Configuration parameters) throws Exception {
                    payValueState = getRuntimeContext().getState(
                        new ValueStateDescriptor<>("pay-event", Types.POJO(Pay.class))
                    );
                    platformValueState = getRuntimeContext().getState(
                        new ValueStateDescriptor<>("third-event", Types.POJO(ThirdPayPlatform.class))
                    );
                }

                @Override
                public void processElement1(Pay value, CoProcessFunction<Pay, ThirdPayPlatform, String>.Context ctx, Collector<String> out) throws Exception {
                    if (Objects.isNull(platformValueState.value())) {
                        payValueState.update(value);
                        ctx.timerService().registerEventTimeTimer(value.getTimestampLong() + 5000L);
                    } else {
                        out.collect("å¯¹è´¦æˆåŠŸ\t" + value.toString() + "\t" + platformValueState.value());
                        platformValueState.clear();
                    }
                }

                @Override
                public void processElement2(ThirdPayPlatform value, CoProcessFunction<Pay, ThirdPayPlatform, String>.Context ctx, Collector<String> out) throws Exception {
                    if (Objects.isNull(payValueState.value())) {
                        platformValueState.update(value);
                        ctx.timerService().registerEventTimeTimer(value.getTime() + 5000L);
                    } else {
                        out.collect("å¯¹è´¦æˆåŠŸ\t" + payValueState.value() + "\t" + value.toString());
                        payValueState.clear();
                    }
                }


                @Override
                public void onTimer(long timestamp, CoProcessFunction<Pay, ThirdPayPlatform, String>.OnTimerContext ctx, Collector<String> out) throws Exception {
                    if (!Objects.isNull(payValueState)) {
                        out.collect("å¯¹è´¦å¤±è´¥\t" + payValueState.value() + "\t" + "NUll");
                    }
                    if (!Objects.isNull(platformValueState)) {
                        out.collect("å¯¹è´¦å¤±è´¥\t" + "NUll" + "\t" + platformValueState.value());
                    }
                    payValueState.clear();
                    platformValueState.clear();
                }
            }).print();

        env.execute();
    }
}

```

#### join çª—å£è¿æ¥

```java
public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    SingleOutputStreamOperator<Pay> paySource = env.fromElements(
        new Pay("order_01", "app", 1000L),
        new Pay("order_02", "app", 1200L),
        new Pay("order_03", "app", 1300L),
        new Pay("order_04", "app", 1400L),
        new Pay("order_05", "app", 1500L)
    ).assignTimestampsAndWatermarks(WatermarkStrategy.<Pay>forBoundedOutOfOrderness(Duration.ofMillis(100))
                                    .withTimestampAssigner((element, recordTimestamp) -> element.getTimestampLong()));


    SingleOutputStreamOperator<ThirdPayPlatform> thirdSource = env.fromElements(
        new ThirdPayPlatform("order_01", "ThirdPayPlatform", true, 1000L - 100L),
        new ThirdPayPlatform("order_02", "ThirdPayPlatform", true, 1200L - 100L),
        new ThirdPayPlatform("order_03", "ThirdPayPlatform", true, 1300L - 100L),
        new ThirdPayPlatform("order_04", "ThirdPayPlatform", true, 1400L - 100L),
        new ThirdPayPlatform("order_05", "ThirdPayPlatform", true, 1500L - 100L)
    ).assignTimestampsAndWatermarks(WatermarkStrategy.<ThirdPayPlatform>forBoundedOutOfOrderness(Duration.ofMillis(100))
                                    .withTimestampAssigner((element, recordTimestamp) -> element.getTime()));

    paySource.join(thirdSource)
        .where(Pay::getOrderId).equalTo(ThirdPayPlatform::getOrderId)
        .window(SlidingEventTimeWindows.of(Time.milliseconds(200), Time.milliseconds(100)))
        .apply((Pay first, ThirdPayPlatform second) -> first + " -> " + second)
        .print();
    env.execute();
}
```

#### intervalJoin é—´éš”è¿æ¥

```java
public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    SingleOutputStreamOperator<Pay> paySource = env.fromElements(
        new Pay("order_01", "app", 1000L),
        new Pay("order_02", "app", 1200L),
        new Pay("order_03", "app", 1300L),
        new Pay("order_04", "app", 1400L),
        new Pay("order_05", "app", 1500L)
    ).assignTimestampsAndWatermarks(WatermarkStrategy.<Pay>forBoundedOutOfOrderness(Duration.ofMillis(100))
                                    .withTimestampAssigner((element, recordTimestamp) -> element.getTimestampLong()));


    SingleOutputStreamOperator<ThirdPayPlatform> thirdSource = env.fromElements(
        new ThirdPayPlatform("order_01", "ThirdPayPlatform", true, 1000L - 100L),
        new ThirdPayPlatform("order_02", "ThirdPayPlatform", true, 1200L - 100L),
        new ThirdPayPlatform("order_03", "ThirdPayPlatform", true, 1300L - 100L),
        new ThirdPayPlatform("order_04", "ThirdPayPlatform", true, 1400L - 100L),
        new ThirdPayPlatform("order_05", "ThirdPayPlatform", true, 1500L - 100L)
    ).assignTimestampsAndWatermarks(WatermarkStrategy.<ThirdPayPlatform>forBoundedOutOfOrderness(Duration.ofMillis(100))
                                    .withTimestampAssigner((element, recordTimestamp) -> element.getTime()));

    paySource.keyBy(Pay::getOrderId).intervalJoin(thirdSource.keyBy(ThirdPayPlatform::getOrderId))
        .between(Time.milliseconds(-250), Time.milliseconds(250))
        .process(new ProcessJoinFunction<Pay, ThirdPayPlatform, String>() {
            @Override
            public void processElement(Pay left, ThirdPayPlatform right, ProcessJoinFunction<Pay, ThirdPayPlatform, String>.Context ctx, Collector<String> out) throws Exception {
                out.collect(left + " -> " + right);
            }
        })
        .print();
    env.execute();
}
```

#### coGroup ç»„åˆçª—å£è¿æ¥

```java
public class ConWindowJoin {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        SingleOutputStreamOperator<Pay> paySource = env.fromElements(
            new Pay("order_01", "app", 1000L), //  900L  750~1000   1000~1500
            new Pay("order_02", "app", 1200L), // 1100L
            new Pay("order_03", "app", 1300L), // 1200L
            new Pay("order_04", "app", 1400L), // 1300L
            new Pay("order_05", "app", 1500L)  // 1400L
        ).assignTimestampsAndWatermarks(WatermarkStrategy.<Pay>forBoundedOutOfOrderness(Duration.ofMillis(100))
                                        .withTimestampAssigner((element, recordTimestamp) -> element.getTimestampLong()));


        SingleOutputStreamOperator<ThirdPayPlatform> thirdSource = env.fromElements(
            new ThirdPayPlatform("order_01", "ThirdPayPlatform", true, 1000L - 100L),
            new ThirdPayPlatform("order_02", "ThirdPayPlatform", true, 1200L - 100L),
            new ThirdPayPlatform("order_03", "ThirdPayPlatform", true, 1300L - 100L),
            new ThirdPayPlatform("order_04", "ThirdPayPlatform", true, 1400L - 100L),
            new ThirdPayPlatform("order_05", "ThirdPayPlatform", true, 1500L - 100L)
        ).assignTimestampsAndWatermarks(WatermarkStrategy.<ThirdPayPlatform>forBoundedOutOfOrderness(Duration.ofMillis(100))
                                        .withTimestampAssigner((element, recordTimestamp) -> element.getTime()));


        paySource.keyBy(Pay::getOrderId).coGroup(thirdSource.keyBy(ThirdPayPlatform::getOrderId))
            .where(Pay::getOrderId, Types.STRING).equalTo(ThirdPayPlatform::getOrderId, Types.STRING)
            .window(SlidingEventTimeWindows.of(Time.milliseconds(500), Time.milliseconds(250)))
            .apply(new CoGroupFunction<Pay, ThirdPayPlatform, String>() {
                @Override
                public void coGroup(Iterable<Pay> first, Iterable<ThirdPayPlatform> second, Collector<String> out) throws Exception {
                    out.collect(first + "->" + second);
                }
            })
            .print();
        env.execute();
    }
}
```

## çŠ¶æ€ç¼–ç¨‹

Flinkç§çš„çŠ¶æ€åˆ†ä¸º`keyed State` å’Œ `Operator State` è¿˜æœ‰å¹¿æ’­çŠ¶æ€`BroadcastState`

### keyed State æŒ‰é”®åˆ†åŒºçŠ¶æ€

å†…ç½®çš„çŠ¶æ€åˆ†ä¸º5ç§ï¼Œåˆ†åˆ«æ˜¯

1. ValueState å€¼çŠ¶æ€

	```java
	getRuntimeContext().getState();
	
	public interface ValueState<T> extends State {
	    // è·å–å½“å‰çŠ¶æ€å€¼
	    T value() throws IOException;
	    
	    // æ›´æ–°ã€è¦†å†™çŠ¶æ€å€¼
	    void update(T value) throws IOException;
	```

2. ListState åˆ—è¡¨çŠ¶æ€

	```java
	getRuntimeContext().getListState();
	
	public interface ListState<T> extends MergingState<T, Iterable<T>> {
	
		// ä¼ å…¥ä¸€ä¸ªåˆ—è¡¨valuesï¼Œç›´æ¥å¯¹çŠ¶æ€è¿›è¡Œè¦†ç›–
	    void update(List<T> values) throws Exception;
	
	    // å‘åˆ—è¡¨ä¸­æ·»åŠ å¤šä¸ªå…ƒç´ ï¼Œä»¥åˆ—è¡¨valueså½¢å¼ä¼ å…¥
	    void addAll(List<T> values) throws Exception;
	    
	    
	    /* AppendingState */
	    // è·å–å½“å‰çš„åˆ—è¡¨çŠ¶æ€ï¼Œè¿”å›çš„æ˜¯ä¸€ä¸ªå¯è¿­ä»£ç±»å‹Iterable<T>
	    Iterable get() throws Exception; 
	
		// åœ¨çŠ¶æ€åˆ—è¡¨ä¸­æ·»åŠ ä¸€ä¸ªå…ƒç´ value
	    void add(IN value) throws Exception;
	```

3. Map State æ˜ å°„çŠ¶æ€

	```java
	public interface MapState<UK, UV> extends State {
	    
		// ä¼ å…¥ä¸€ä¸ªä¸ªkeyä½œä¸ºå‚æ•°ï¼ŒæŸ¥è¯¢å¯¹åº”çš„valueå€¼
	    UV get(UK key) throws Exception;
	    
		// ä¼ å…¥ä¸€ä¸ªé”®å€¼å¯¹ï¼Œæ›´æ–°keyå¯¹åº”çš„valueå€¼
	    void put(UK key, UV value) throws Exception;
	    
	    // ä¼ å…¥æ˜ å°„mapä¸­æ‰€æœ‰çš„é”®å€¼å¯¹ï¼Œå…¨éƒ¨æ·»åŠ åˆ°æ˜ å°„çŠ¶æ€ä¸­
	    void putAll(Map<UK, UV> map) throws Exception;
	
		// åˆ é™¤æŒ‡å®škey
	    void remove(UK key) throws Exception;
	
	    // åˆ¤æ–­æ˜¯å¦å­˜åœ¨æŒ‡å®šçš„key,è¿”å›ä¸€ä¸ªbooleanå€¼
	    boolean contains(UK key) throws Exception;
	
	    // è·å–æ˜ å°„çŠ¶æ€ä¸­æ‰€æœ‰çš„é”®å€¼å¯¹
	    Iterable<Map.Entry<UK, UV>> entries() throws Exception;
	
	    // è·å–æ˜ å°„çŠ¶æ€ä¸­æ‰€æœ‰çš„é”®(key),è¿”å›ä¸€ä¸ªå¯è¿­ä»£Iterableç±»å‹
	    Iterable<UK> keys() throws Exception;
	
	    // è·å–æ˜ å°„çŠ¶æ€ä¸­æ‰€æœ‰çš„å€¼(value),è¿”å›ä¸€ä¸ªå¯è¿­ä»£Iterableç±»å‹
	    Iterable<UV> values() throws Exception;
	
	    // è·å–æ˜ å°„çŠ¶æ€ä¸­æ‰€æœ‰çš„é”®å€¼å¯¹
	    Iterator<Map.Entry<UK, UV>> iterator() throws Exception;
	
	    boolean isEmpty() throws Exception;
	}
	```

4. ReducingState

	```java
	// çŠ¶æ€ä¸­ä¿å­˜çš„æ˜¯ç»“æœï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„å€¼ï¼Œç»“æœæ˜¯ç”±åˆå§‹ç»“æœä¸æ‰€æœ‰çš„å€¼ï¼ŒæŒ‰ç…§é€»è¾‘è¿­ä»£è€Œæ¥
	// ç»§æ‰¿äº†ReduceFunctionçš„ç‰¹ç‚¹ (V, V) -> V
	public ReducingStateDescriptor ( String name, ReduceFunction<T> reduceFunction, Class<T> typeClass) {}
	/*
	ç±»ä¼¼äºå€¼çŠ¶æ€(Value)ï¼Œä¸è¿‡éœ€è¦å¯¹æ·»åŠ è¿›æ¥çš„æ‰€æœ‰æ•°æ®è¿›è¡Œå½’çº¦ï¼Œå°†å½’çº¦èšåˆä¹‹åçš„å€¼ä½œä¸ºçŠ¶æ€ä¿å­˜ä¸‹æ¥ã€‚ReducintState<T>è¿™ ä¸ªæ¥å£è°ƒç”¨çš„æ–¹æ³•ç±»ä¼¼äºListStateï¼Œ åªä¸è¿‡å®ƒä¿å­˜çš„åªæ˜¯ä¸€ä¸ªèšåˆå€¼ï¼Œæ‰€ä»¥è°ƒç”¨.add(æ–¹æ³•æ—¶ï¼Œä¸æ˜¯åœ¨çŠ¶æ€åˆ—è¡¨é‡Œæ·»åŠ å…ƒç´ ï¼Œè€Œæ˜¯ç›´æ¥æŠŠæ–°æ•°æ®å’Œä¹‹å‰çš„çŠ¶æ€è¿›è¡Œå½’çº¦ï¼Œå¹¶ç”¨å¾—åˆ°çš„ç»“æœæ›´æ–°çŠ¶æ€
	*/
	```

5. AggregatingState

	```java
	// çŠ¶æ€ä¸­ä¿å­˜çš„æ˜¯ç»“æœï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„å€¼ï¼Œç»“æœæ˜¯ç”±åˆå§‹ç»“æœä¸æ‰€æœ‰çš„å€¼ï¼ŒæŒ‰ç…§é€»è¾‘è¿­ä»£è€Œæ¥
	// ç»§æ‰¿äº†AggregatingStateçš„ç‰¹ç‚¹æœ‰é€šç”¨çš„é€»è¾‘
	
	/* 
	AggregatingState()ä¸å½’çº¦çŠ¶æ€éå¸¸ç±»ä¼¼ï¼ŒèšåˆçŠ¶æ€ä¹Ÿæ˜¯ä¸€ä¸ªå€¼ï¼Œç”¨æ¥ä¿å­˜æ·»åŠ è¿›æ¥çš„æ‰€æœ‰æ•°æ®çš„èšåˆç»“æœã€‚ä¸ReducingState ä¸åŒçš„æ˜¯ï¼Œå®ƒçš„èšåˆé€»è¾‘æ˜¯ç”±åœ¨æè¿°å™¨ä¸­ä¼ å…¥ä¸€ä¸ªæ›´åŠ ä¸€èˆ¬åŒ–çš„èšåˆå‡½æ•°(AggregateFunction)æ¥å®šä¹‰çš„;è¿™ä¹Ÿå°±æ˜¯ä¹‹å‰æˆ‘ä»¬è®²è¿‡çš„AggregateFunctionï¼Œ é‡Œé¢é€šè¿‡ä¸€ä¸ªç´¯åŠ å™¨(Accumulator)æ¥è¡¨ç¤ºçŠ¶æ€ï¼Œæ‰€ä»¥èšåˆçš„çŠ¶æ€ç±»å‹å¯ä»¥è·Ÿæ·»åŠ è¿›æ¥çš„æ•°æ®ç±»å‹å®Œå…¨ä¸åŒï¼Œä½¿ç”¨æ›´åŠ çµæ´»ã€‚
	*/
	```

### Operator State ç®—å­çŠ¶æ€



### <a name="boarodcastjoin">BroadcastState å¹¿æ’­çŠ¶æ€</a>





















 
